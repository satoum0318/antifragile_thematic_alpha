# -*- coding: utf-8 -*-
"""
J-Quants åé›†â†’å‡çµã‚­ãƒ£ãƒƒã‚·ãƒ¥â†’å®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³åˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- 4æ—¥é‹ç”¨: åé›†(ä¾¡æ ¼+è²¡å‹™)ã‚’æ—¥æ¬¡800reqå†…ã§é€²ã‚ã€4æ—¥ç›®ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ä¸€æ‹¬åˆ†æ
- ãƒ¢ãƒƒã‚¯ä¸ä½¿ç”¨: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ™‚ã¯â€œè¨ˆç®—ä¸èƒ½ã¯Noneâ€ã§è¿”ã™ï¼ˆæ¨å®šã‚„ãƒ©ãƒ³ãƒ€ãƒ ã¯è¡Œã‚ãªã„ï¼‰
- ç«¯æœ«å¯¾è©±ãƒ¡ãƒ‹ãƒ¥ãƒ¼ä»˜ãï¼ˆå¼•æ•°æœªæŒ‡å®šã§èµ·å‹•ã™ã‚‹ã¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤ºï¼‰
- CLIå¯¾å¿œ:
    åé›†:   python script.py --phase collect --budget 380
    è§£æ:   python script.py --phase analyze --top 10
    å˜éŠ˜æŸ„: python script.py --phase single --code 8035
    å…¨ä»¶:   python script.py --phase collect_all --budget 380
ç’°å¢ƒå¤‰æ•°:
    JQ_RPM=50  JQ_RPD=800  # å¿…è¦ãªã‚‰èª¿æ•´

è¿½åŠ ï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®ã€Œå¿…é ˆã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’å®Ÿè£…ï¼‰
- æµå‹•æ€§ãƒ•ã‚£ãƒ«ã‚¿: avg_volume_30d >= MIN_AVG_VOLUME_30D ã‹ã¤ market_cap >= MIN_MARKET_CAP_JPY
- ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³å¥å…¨æ€§: PS<=MAX_PS_DEFENSIVE ã‚’ core æ¡ä»¶ã€PER>MAX_PER_CORE ã¯ satellite æ‰±ã„
- åç›Šå®‰å®šæ€§ï¼ˆå–¶æ¥­åˆ©ç›Šï¼‰: ç›´è¿‘å¹´ã§èµ¤å­—ã‚’å«ã¾ãªã„ + ç›´è¿‘ãŒæ€¥è½ã—ã¦ã„ãªã„

ãƒ•ã‚£ãƒ«ã‚¿ã¯ core/satellite/excluded ã®3åˆ†é¡ã¨ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã«ã‚‚åæ˜ ã—ã¾ã™ã€‚

å¿…è¦: pandas, numpy, requests
"""

from __future__ import annotations

import os
import re
import sys
import json
import time
import signal
import logging
import datetime
import configparser
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
import requests

# ------------------------------------------------------------
# ãƒ­ã‚®ãƒ³ã‚°
# ------------------------------------------------------------
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(logging.NullHandler())

# ------------------------------------------------------------
# å®šæ•°ãƒ»ãƒ‘ã‚¹
# ------------------------------------------------------------
JQUANTS_API_BASE = "https://api.jquants.com/v1"
CACHE_DIR = Path(".jquants_cache")
CACHE_DIR.mkdir(exist_ok=True)
LOOKBACK_DAYS = 700
REPORTS_DIR = Path("output") / "reports"

# ------------------------------------------------------------
# ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¿…é ˆãƒ•ã‚£ãƒ«ã‚¿ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰
# ------------------------------------------------------------
MIN_AVG_VOLUME_30D = int(os.getenv("MIN_AVG_VOLUME_30D", "50000"))
MIN_MARKET_CAP_JPY = int(os.getenv("MIN_MARKET_CAP_JPY", "50000000000"))  # 50B JPY
MAX_PS_DEFENSIVE = float(os.getenv("MAX_PS_DEFENSIVE", "2.0"))
MAX_PER_CORE = float(os.getenv("MAX_PER_CORE", "60.0"))

# åç›Šå®‰å®šæ€§ï¼ˆå–¶æ¥­åˆ©ç›Šï¼‰åˆ¤å®š
OP_INCOME_YEARS = int(os.getenv("OP_INCOME_YEARS", "3"))                # ç›´è¿‘ä½•å¹´è¦‹ã‚‹ã‹ï¼ˆæ–°ã—ã„å¹´åº¦é †ï¼‰
OP_INCOME_DROP_FLOOR = float(os.getenv("OP_INCOME_DROP_FLOOR", "0.3"))  # ç›´è¿‘å–¶æ¥­åˆ©ç›ŠãŒéå»å¹´ä¸­å¤®å€¤ã®ä½•å€ä»¥ä¸Šãªã‚‰OK
EXCLUDE_OP_INCOME_DEFICIT = (os.getenv("EXCLUDE_OP_INCOME_DEFICIT", "1") != "0")  # ç›´è¿‘å¹´ã«èµ¤å­—ãŒã‚ã‚Œã°é™¤å¤–ï¼ˆãƒ‡ãƒ•ã‚©ONï¼‰

# ------------------------------------------------------------
# ãƒ˜ãƒ«ãƒ‘
# ------------------------------------------------------------
def seconds_until_next_day(buffer_sec: int = 10) -> int:
    now = datetime.datetime.now()
    tomorrow = now + datetime.timedelta(days=1)
    reset = tomorrow.replace(hour=0, minute=0, second=0, microsecond=0)
    return max(1, int((reset - now).total_seconds()) + buffer_sec)

def build_prices_endpoint(stock_code: str, lookback_days: int = LOOKBACK_DAYS) -> str:
    start = (datetime.date.today() - datetime.timedelta(days=lookback_days)).strftime("%Y-%m-%d")
    return f"prices/daily_quotes?code={stock_code}&from={start}"

# ------------------------------------------------------------
# Graceful Shutdown
# ------------------------------------------------------------
class GracefulShutdown:
    def __init__(self):
        self.shutdown = False
        try:
            signal.signal(signal.SIGINT, self.exit_gracefully)
            signal.signal(signal.SIGTERM, self.exit_gracefully)
        except Exception:
            pass

    def exit_gracefully(self, signum, frame):
        print(f"\nâš ï¸ ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«å—ä¿¡: {signum}\nğŸ›‘ å®‰å…¨ã«çµ‚äº†ã—ã¾ã™")
        self.shutdown = True
        sys.exit(130)

graceful_shutdown = GracefulShutdown()

# ------------------------------------------------------------
# ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒã‚¿ + èªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³
# ------------------------------------------------------------
class APIRateLimiter:
    """J-Quants 50 req/min, 800 req/day ã‚’æƒ³å®š"""
    def __init__(self, rpm: int = 50, rpd: int = 800):
        self.requests_per_minute = rpm
        self.requests_per_day = rpd
        self.base_delay = 1.5
        self.request_timestamps: List[datetime.datetime] = []
        self.daily_count = 0
        self.last_reset = datetime.date.today()

    def wait_if_needed(self):
        now = datetime.datetime.now()
        if now.date() > self.last_reset:
            self.daily_count = 0
            self.last_reset = now.date()

        if self.daily_count >= self.requests_per_day:
            raise RuntimeError("æ—¥æ¬¡ãƒ¬ãƒ¼ãƒˆåˆ¶é™åˆ°é”")

        one_minute_ago = now - datetime.timedelta(minutes=1)
        self.request_timestamps = [t for t in self.request_timestamps if t > one_minute_ago]

        if len(self.request_timestamps) >= self.requests_per_minute:
            wait = 61 - (now - min(self.request_timestamps)).total_seconds()
            if wait > 0:
                time.sleep(wait)

        time.sleep(self.base_delay)

    def mark(self):
        now = datetime.datetime.now()
        self.request_timestamps.append(now)
        self.daily_count += 1

class AuthSession(requests.Session):
    """J-Quants èªè¨¼ï¼‹ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œSession"""
    def __init__(self, limiter: APIRateLimiter, ini_file: str = "api.ini"):
        super().__init__()
        self.limiter = limiter
        self.ini_file = ini_file

    def request(self, method, url, **kwargs):
        MAX = 5
        timeout = kwargs.pop("timeout", 30)

        for attempt in range(1, MAX + 1):
            self.limiter.wait_if_needed()
            try:
                resp = super().request(method, url, timeout=timeout, **kwargs)
            except requests.RequestException:
                if attempt == MAX:
                    raise
                time.sleep(1.5 * attempt)
                continue

            # 401: idToken refresh
            if resp.status_code == 401 and attempt == 1:
                _refresh_id_token(self, ini_file=self.ini_file)
                continue

            # Retryable
            if resp.status_code in (429,) or resp.status_code >= 500:
                if attempt == MAX:
                    return resp
                time.sleep(min(2 ** attempt, 30))
                continue

            self.limiter.mark()
            return resp

        raise RuntimeError(f"{method} {url} failed after {MAX} attempts")

def get_authenticated_session_jquants(ini_file: str = "api.ini") -> requests.Session:
    token_cache = CACHE_DIR / "access_token.json"
    rpm = int(os.getenv("JQ_RPM", "50"))
    rpd = int(os.getenv("JQ_RPD", "800"))
    limiter = APIRateLimiter(rpm=rpm, rpd=rpd)
    session = AuthSession(limiter, ini_file=ini_file)

    if token_cache.exists():
        try:
            cached = json.loads(token_cache.read_text(encoding="utf-8"))
            exp = datetime.datetime.strptime(cached["expires_at"], "%Y-%m-%dT%H:%M:%S").replace(tzinfo=datetime.timezone.utc)
            if datetime.datetime.now(datetime.timezone.utc) < exp:
                session.headers.update({"Authorization": f"Bearer {cached['token']}"})
                print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥idTokenã‚’ä½¿ç”¨")
                return session
        except Exception:
            pass

    print("ğŸ”‘ èªè¨¼é–‹å§‹â€¦")
    _refresh_id_token(session, ini_file=ini_file)
    print("âœ… èªè¨¼æˆåŠŸ")
    return session

def _refresh_id_token(session: requests.Session, ini_file: str = "api.ini") -> str:
    config = configparser.ConfigParser()
    config.read(ini_file, encoding="utf-8")

    email = (config["DEFAULT"].get("MAIL_ADDRESS") or
             config["DEFAULT"].get("mail_address") or
             config["DEFAULT"].get("email"))
    password = (config["DEFAULT"].get("PASSWORD") or
                config["DEFAULT"].get("password"))

    if not (email and password):
        raise RuntimeError("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æœªè¨­å®š(api.ini)")

    auth_payload = {"mailaddress": email, "password": password}
    res = requests.post(f"{JQUANTS_API_BASE}/token/auth_user", json=auth_payload, timeout=20)
    res.raise_for_status()
    refresh_token = res.json().get("refreshToken")
    if not refresh_token:
        raise RuntimeError("refreshTokenå–å¾—å¤±æ•—")

    tok_res = requests.post(f"{JQUANTS_API_BASE}/token/auth_refresh?refreshtoken={refresh_token}", timeout=20)
    tok_res.raise_for_status()
    id_token = tok_res.json().get("idToken")
    if not id_token:
        raise RuntimeError("idTokenå–å¾—å¤±æ•—")

    session.headers.update({"Authorization": f"Bearer {id_token}"})
    expires = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(hours=23)
    (CACHE_DIR / "access_token.json").write_text(
        json.dumps({"token": id_token, "expires_at": expires.strftime("%Y-%m-%dT%H:%M:%S")}, ensure_ascii=False),
        encoding="utf-8"
    )
    return id_token

# ------------------------------------------------------------
# æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå‡çµï¼‰
# ------------------------------------------------------------
class FrozenCache:
    BASE = CACHE_DIR / "frozen"
    def __init__(self):
        (self.BASE / "prices").mkdir(parents=True, exist_ok=True)
        (self.BASE / "statements").mkdir(parents=True, exist_ok=True)

    def prices_path(self, code: str) -> Path:
        return self.BASE / "prices" / f"{code}.csv"

    def stmts_path(self, code: str) -> Path:
        return self.BASE / "statements" / f"{code}.json"

    def save_prices(self, code: str, df: pd.DataFrame) -> None:
        if df is None or df.empty:
            return
        df.to_csv(self.prices_path(code), index=False)

    def load_prices(self, code: str) -> Optional[pd.DataFrame]:
        p = self.prices_path(code)
        if not p.exists():
            return None
        try:
            return pd.read_csv(p)
        except Exception:
            return None

    def save_statements(self, code: str, stmts: List[dict]) -> None:
        self.stmts_path(code).write_text(json.dumps({"statements": stmts}, ensure_ascii=False), encoding="utf-8")

    def load_statements(self, code: str) -> List[dict]:
        p = self.stmts_path(code)
        if not p.exists():
            return []
        try:
            return json.loads(p.read_text(encoding="utf-8")).get("statements", [])
        except Exception:
            return []

    def has_all(self, code: str, max_age_days: Optional[int] = None) -> bool:
        p1, p2 = self.prices_path(code), self.stmts_path(code)
        if not p1.exists() or not p2.exists():
            return False
        if max_age_days is None:
            return True
        try:
            now = time.time()
            age_days_prices = (now - p1.stat().st_mtime) / 86400.0
            age_days_stmts  = (now - p2.stat().st_mtime) / 86400.0
            oldest = max(age_days_prices, age_days_stmts)
            return oldest <= max_age_days
        except Exception:
            return True

# ------------------------------------------------------------
# ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ãƒ»éŠ˜æŸ„ãƒªã‚¹ãƒˆ
# ------------------------------------------------------------
class DynamicSectorAverages:
    SECTOR_MEDIANS = {
        "é›»æ°—æ©Ÿå™¨": {"ca_ratio": 0.62, "cl_ratio": 0.38, "gpm": 0.31},
        "åŠå°ä½“":   {"ca_ratio": 0.55, "cl_ratio": 0.42, "gpm": 0.39},
        "éŠ€è¡Œ":     {"ca_ratio": 0.28, "cl_ratio": 0.90, "gpm": 0.20},
        "æƒ…å ±ãƒ»é€šä¿¡æ¥­": {"ca_ratio": 0.57, "cl_ratio": 0.32, "gpm": 0.34},
        "ã‚µãƒ¼ãƒ“ã‚¹": {"ca_ratio": 0.60, "cl_ratio": 0.35, "gpm": 0.29},
        "åŒ–å­¦":     {"ca_ratio": 0.58, "cl_ratio": 0.37, "gpm": 0.27},
        "ãã®ä»–":   {"ca_ratio": 0.60, "cl_ratio": 0.40, "gpm": 0.25},
    }

    def __init__(self, session: requests.Session):
        self.session = session
        self.sector_cache: dict = {}
        self.cache_timestamp: Optional[float] = None
        self.cache_duration = 3600

    @staticmethod
    def normalize_sector(sector: str) -> str:
        s = (sector or "").strip()
        if not s:
            return "ãã®ä»–"
        if "éŠ€è¡Œ" in s:
            return "éŠ€è¡Œ"
        if "æƒ…å ±" in s or "é€šä¿¡" in s:
            return "æƒ…å ±ãƒ»é€šä¿¡æ¥­"
        if "é›»æ°—æ©Ÿå™¨" in s:
            return "é›»æ°—æ©Ÿå™¨"
        if "è¼¸é€ç”¨æ©Ÿå™¨" in s or "è‡ªå‹•è»Š" in s:
            return "è‡ªå‹•è»Š"
        if "ã‚µãƒ¼ãƒ“ã‚¹" in s:
            return "ã‚µãƒ¼ãƒ“ã‚¹"
        if "åŒ–å­¦" in s:
            return "åŒ–å­¦"
        return s if s in DynamicSectorAverages.SECTOR_MEDIANS else "ãã®ä»–"

    @staticmethod
    def get_sector_static(stock_code: str) -> str:
        sector_mapping = {
            '7203': 'è‡ªå‹•è»Š','7267':'è‡ªå‹•è»Š','7269':'è‡ªå‹•è»Š','7270':'è‡ªå‹•è»Š','7261':'è‡ªå‹•è»Š','7202':'è‡ªå‹•è»Š','7211':'è‡ªå‹•è»Š',
            '8035':'åŠå°ä½“','6861':'åŠå°ä½“','6594':'åŠå°ä½“','6503':'åŠå°ä½“','6723':'åŠå°ä½“','6752':'åŠå°ä½“','6981':'åŠå°ä½“',
            '6758':'é›»æ°—æ©Ÿå™¨','6501':'é›»æ°—æ©Ÿå™¨','6954':'é›»æ°—æ©Ÿå™¨','6702':'é›»æ°—æ©Ÿå™¨','6976':'é›»æ°—æ©Ÿå™¨',
            '8306':'éŠ€è¡Œ','8316':'éŠ€è¡Œ','8411':'éŠ€è¡Œ','8331':'éŠ€è¡Œ','8354':'éŠ€è¡Œ','8393':'éŠ€è¡Œ',
            '9984':'æƒ…å ±ãƒ»é€šä¿¡æ¥­','9432':'æƒ…å ±ãƒ»é€šä¿¡æ¥­','9433':'æƒ…å ±ãƒ»é€šä¿¡æ¥­','4689':'æƒ…å ±ãƒ»é€šä¿¡æ¥­','3659':'æƒ…å ±ãƒ»é€šä¿¡æ¥­','4751':'æƒ…å ±ãƒ»é€šä¿¡æ¥­',
            '4568':'åŒ»è–¬å“','4519':'åŒ»è–¬å“','4523':'åŒ»è–¬å“','4503':'åŒ»è–¬å“','4506':'åŒ»è–¬å“','4507':'åŒ»è–¬å“',
            '8058':'å•†ç¤¾','8031':'å•†ç¤¾','2768':'å•†ç¤¾','8002':'å•†ç¤¾','8001':'å•†ç¤¾','8053':'å•†ç¤¾',
            '9983':'å°å£²','3382':'å°å£²','8267':'å°å£²','3086':'å°å£²','3099':'å°å£²','8233':'å°å£²',
            '4661':'ã‚µãƒ¼ãƒ“ã‚¹','9602':'ã‚µãƒ¼ãƒ“ã‚¹','2432':'ã‚µãƒ¼ãƒ“ã‚¹','4324':'ã‚µãƒ¼ãƒ“ã‚¹','6178':'ã‚µãƒ¼ãƒ“ã‚¹',
            '7974':'ã‚²ãƒ¼ãƒ ','9684':'ã‚²ãƒ¼ãƒ ','7832':'ã‚²ãƒ¼ãƒ ','3765':'ã‚²ãƒ¼ãƒ ',
            '4901':'åŒ–å­¦','4452':'åŒ–å­¦','4063':'åŒ–å­¦','4005':'åŒ–å­¦','4188':'åŒ–å­¦','4183':'åŒ–å­¦',
            '5401':'é‰„é‹¼','5411':'é‰„é‹¼','5406':'é‰„é‹¼','5423':'é‰„é‹¼',
            '8801':'ä¸å‹•ç”£','8802':'ä¸å‹•ç”£','3289':'ä¸å‹•ç”£','1928':'ä¸å‹•ç”£',
            '2914':'é£Ÿå“','2502':'é£Ÿå“','2269':'é£Ÿå“','2503':'é£Ÿå“','2801':'é£Ÿå“','2871':'é£Ÿå“',
            '6367':'æ©Ÿæ¢°','6473':'æ©Ÿæ¢°','6326':'æ©Ÿæ¢°',
            '9020':'é‹è¼¸','9021':'é‹è¼¸','9022':'é‹è¼¸','9101':'é‹è¼¸','9104':'é‹è¼¸','9107':'é‹è¼¸',
            '8473':'è¨¼åˆ¸','8601':'è¨¼åˆ¸','8604':'è¨¼åˆ¸',
            '5020':'ã‚¨ãƒãƒ«ã‚®ãƒ¼','1605':'ã‚¨ãƒãƒ«ã‚®ãƒ¼','1662':'ã‚¨ãƒãƒ«ã‚®ãƒ¼',
            '1801':'å»ºè¨­','1802':'å»ºè¨­','1803':'å»ºè¨­','1928':'å»ºè¨­',
        }
        return sector_mapping.get(stock_code, "ãã®ä»–")

    def is_cache_valid(self) -> bool:
        if not self.cache_timestamp:
            return False
        return (time.time() - self.cache_timestamp) < self.cache_duration

    def get_default_sector_average(self, sector: str) -> dict:
        defaults = {
            'è‡ªå‹•è»Š': {'ps': 0.8, 'peg': 1.2, 'eps_growth': 8.5},
            'åŠå°ä½“': {'ps': 4.5, 'peg': 1.8, 'eps_growth': 12.2},
            'é›»æ°—æ©Ÿå™¨': {'ps': 1.8, 'peg': 1.5, 'eps_growth': 12.3},
            'éŠ€è¡Œ': {'ps': 2.5, 'peg': 0.8, 'eps_growth': 10.6},
            'æƒ…å ±ãƒ»é€šä¿¡æ¥­': {'ps': 1.2, 'peg': 1.3, 'eps_growth': 11.2},
            'åŒ»è–¬å“': {'ps': 3.8, 'peg': 1.6, 'eps_growth': 10.5},
            'å•†ç¤¾': {'ps': 0.4, 'peg': 0.9, 'eps_growth': 10.2},
            'å°å£²': {'ps': 0.8, 'peg': 1.4, 'eps_growth': 11.1},
            'ã‚µãƒ¼ãƒ“ã‚¹': {'ps': 2.2, 'peg': 1.7, 'eps_growth': 12.1},
            'ã‚²ãƒ¼ãƒ ': {'ps': 3.5, 'peg': 1.4, 'eps_growth': 12.3},
            'åŒ–å­¦': {'ps': 1.0, 'peg': 1.4, 'eps_growth': 9.1},
            'ãã®ä»–': {'ps': 1.5, 'peg': 1.5, 'eps_growth': 10.0},
        }
        default = defaults.get(sector, defaults['ãã®ä»–'])
        return {
            **default,
            'sample_count': 0,
            'last_updated': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'data_source': 'static_default'
        }

    def get_sector_averages(self, force_refresh: bool = False) -> dict:
        if not force_refresh and self.is_cache_valid() and self.sector_cache:
            print("ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥")
            return self.sector_cache

        cache_file = CACHE_DIR / "sector_averages.json"
        if cache_file.exists() and not force_refresh:
            try:
                j = json.loads(cache_file.read_text(encoding="utf-8"))
                if time.time() - j.get("timestamp", 0) <= 86400:
                    self.sector_cache = j.get("data", {})
                    self.cache_timestamp = time.time()
                    print("ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡: ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥")
                    return self.sector_cache
            except Exception:
                pass

        print("ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡: é™çš„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ")
        sectors = ['è‡ªå‹•è»Š','åŠå°ä½“','é›»æ°—æ©Ÿå™¨','éŠ€è¡Œ','æƒ…å ±ãƒ»é€šä¿¡æ¥­','åŒ»è–¬å“','å•†ç¤¾','å°å£²','ã‚µãƒ¼ãƒ“ã‚¹','ã‚²ãƒ¼ãƒ ','åŒ–å­¦','ãã®ä»–']
        data = {s: self.get_default_sector_average(s) for s in sectors}
        cache_file.write_text(json.dumps({"timestamp": time.time(), "data": data}, ensure_ascii=False), encoding="utf-8")
        self.sector_cache = data
        self.cache_timestamp = time.time()
        return data

    def load_or_download_data_v2(self, endpoint: str, cache_name: str, bypass_cache: bool = False) -> pd.DataFrame:
        """å½“æ—¥CSVã‚­ãƒ£ãƒƒã‚·ãƒ¥â†’APIâ†’CSVä¿å­˜ã€‚bypass_cache=True ãªã‚‰å½“æ—¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å–ã‚Šç›´ã™ã€‚"""
        try:
            today = datetime.date.today().strftime("%Y%m%d")
            cache_file = CACHE_DIR / f"{cache_name}_{today}.csv"

            if cache_file.exists() and not bypass_cache:
                try:
                    df = pd.read_csv(cache_file)
                    if not df.empty:
                        return df
                except Exception:
                    pass

            url = f"{JQUANTS_API_BASE}/{endpoint}"
            res = self.session.get(url, timeout=30)
            if res.status_code != 200:
                return pd.DataFrame()

            response_data = res.json()
            keys = ["info", "daily_quotes", "statements", "data", "results", "items", "companies", "stocks"]
            data = None
            for k in keys:
                if k in response_data and response_data[k]:
                    data = response_data[k]
                    break
            if data is None:
                data = response_data

            if isinstance(data, list) and len(data) > 0:
                df = pd.DataFrame(data)
                try:
                    df.to_csv(cache_file, index=False)
                except Exception:
                    pass
                return df

            return pd.DataFrame()
        except Exception:
            return pd.DataFrame()

    def get_fallback_stock_list_v2(self) -> list[dict]:
        return [
            {"Code":"7203","CompanyName":"ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š","Sector33Name":"è¼¸é€ç”¨æ©Ÿå™¨","MarketCode":"111"},
            {"Code":"8306","CompanyName":"ä¸‰è±UFJãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«G","Sector33Name":"éŠ€è¡Œæ¥­","MarketCode":"111"},
            {"Code":"6758","CompanyName":"ã‚½ãƒ‹ãƒ¼G","Sector33Name":"é›»æ°—æ©Ÿå™¨","MarketCode":"111"},
            {"Code":"9984","CompanyName":"ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯G","Sector33Name":"æƒ…å ±ãƒ»é€šä¿¡æ¥­","MarketCode":"111"},
            {"Code":"8035","CompanyName":"æ±äº¬ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ³","Sector33Name":"é›»æ°—æ©Ÿå™¨","MarketCode":"111"},
        ]

    def get_stock_list_v2(self, force_refresh: bool = False) -> pd.DataFrame:
        try:
            today = datetime.date.today().strftime("%Y%m%d")
            cache_file = CACHE_DIR / f"sector_stock_list_{today}.csv"
            if cache_file.exists() and not force_refresh:
                return pd.read_csv(cache_file)

            print("ğŸ“‹ éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—â€¦")
            df = self.load_or_download_data_v2("listed/info", "sector_listed_info")
            if not df.empty:
                if "Code" in df.columns:
                    df["Code"] = df["Code"].astype(str).str.extract(r"(\d{4})", expand=False)
                    df = df.dropna(subset=["Code"])
                    df = df[df["Code"].str.isdigit()].drop_duplicates("Code")
                df = enhance_stock_list_with_sectors(df)
                df.to_csv(cache_file, index=False)
                return df

            fb = pd.DataFrame(self.get_fallback_stock_list_v2())
            fb = enhance_stock_list_with_sectors(fb)
            fb.to_csv(cache_file, index=False)
            return fb
        except Exception:
            fb = pd.DataFrame(self.get_fallback_stock_list_v2())
            fb = enhance_stock_list_with_sectors(fb)
            return fb

    def calculate_sector_averages_from_cache(self, max_samples_per_sector: int = 100) -> dict:
        try:
            tasks = build_offline_analysis_tasks(self.session)
            if not tasks:
                print("ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡è¨ˆç®—: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return {}

            print(f"ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡è¨ˆç®—: {len(tasks)}éŠ˜æŸ„ã‹ã‚‰è¨ˆç®—ä¸­...")
            results = []
            max_workers = max(4, min(16, (os.cpu_count() or 4) * 2))

            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futs = [
                    ex.submit(
                        analyze_single_stock_complete_v3,
                        self.session, {}, code, name, market, sector,
                        offline=True
                    ) for (code, name, market, sector) in tasks[:max_samples_per_sector * 20]
                ]
                for i, fut in enumerate(as_completed(futs), 1):
                    res = fut.result()
                    if res.get("success") and res.get("ps_ratio") is not None:
                        results.append(res)
                    if i % 100 == 0:
                        print(f"  â± {i}/{len(futs)} å®Œäº† (æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿={len(results)})")

            if not results:
                print("ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡è¨ˆç®—: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return {}

            df = pd.DataFrame([
                {
                    "sector": self.normalize_sector(r.get("sector_name") or ""),
                    "ps": r.get("ps_ratio"),
                    "peg": r.get("peg_ratio"),
                    "per": r.get("per"),
                }
                for r in results
            ])

            sector_stats = {}
            for sector in df["sector"].unique():
                sector_df = df[df["sector"] == sector]
                if len(sector_df) < 3:
                    continue

                ps_values = sector_df["ps"].dropna()
                peg_values = sector_df["peg"].dropna()
                per_values = sector_df["per"].dropna()

                sector_stats[sector] = {
                    "ps": float(ps_values.median()) if len(ps_values) > 0 else None,
                    "peg": float(peg_values.median()) if len(peg_values) > 0 else None,
                    "per": float(per_values.median()) if len(per_values) > 0 else None,
                    "sample_count": len(sector_df),
                    "last_updated": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "data_source": "calculated_from_cache"
                }

            print(f"ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡è¨ˆç®—å®Œäº†: {len(sector_stats)}ã‚»ã‚¯ã‚¿ãƒ¼")
            return sector_stats

        except Exception as e:
            print(f"âš ï¸ ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return {}

# ------------------------------------------------------------
# FinancialDataManagerï¼ˆæœ€å°ï¼‰
# ------------------------------------------------------------
class FinancialDataManager:
    def __init__(self, session: requests.Session):
        self.session = session
        self.base_url = JQUANTS_API_BASE
        self.cache_dir = CACHE_DIR

    def get_stock_list_v2(self, force_refresh: bool = False) -> pd.DataFrame:
        helper = DynamicSectorAverages(self.session)
        return helper.get_stock_list_v2(force_refresh=force_refresh)

    def load_or_download_data_v2(self, endpoint: str, cache_name: str) -> pd.DataFrame:
        helper = DynamicSectorAverages(self.session)
        return helper.load_or_download_data_v2(endpoint, cache_name)

    def _load_json_cached(self, endpoint: str, cache_name: str, ttl_hours: int = 24) -> dict:
        f = self.cache_dir / f"{cache_name}.json"
        if f.exists():
            mtime = datetime.datetime.fromtimestamp(f.stat().st_mtime)
            if (datetime.datetime.now() - mtime).total_seconds() < ttl_hours * 3600:
                try:
                    return json.loads(f.read_text(encoding="utf-8"))
                except Exception:
                    pass

        url = f"{self.base_url}/{endpoint}"
        try:
            res = self.session.get(url, timeout=30)
            if res.status_code == 200:
                data = res.json()
                try:
                    f.write_text(json.dumps(data, ensure_ascii=False), encoding="utf-8")
                except Exception:
                    pass
                return data
            return {}
        except Exception:
            return {}

    def fetch_statements(self, code: str, force_refresh: bool = False) -> List[dict]:
        cache_key = f"fins_statements_{code}"
        if not force_refresh:
            cached = self._load_json_cached(f"fins/statements?code={code}", cache_key, ttl_hours=12)
            if cached and cached.get("statements"):
                return cached["statements"]

        url = f"{self.base_url}/fins/statements?code={code}"
        for attempt in range(1, 6):
            resp = self.session.get(url, timeout=30)
            status = resp.status_code
            try:
                data = resp.json()
                stmts = data.get("statements", [])
            except Exception:
                stmts = []
                data = {}

            if status == 200:
                try:
                    (self.cache_dir / f"{cache_key}.json").write_text(
                        json.dumps(data, ensure_ascii=False, separators=(",", ":")),
                        encoding="utf-8"
                    )
                except Exception:
                    pass
                return stmts

            if status in (402, 403):
                return []

            if status == 429:
                time.sleep(2 ** attempt)
                continue

            if status >= 500:
                time.sleep(1.5 * attempt)
                continue

        return []

    def _fill_missing_fields(self, fin: dict) -> dict:
        cur, prev = fin.get("current", {}), fin.get("previous", {})

        for fld in ("current_assets", "current_liabilities", "gross_profit_margin", "shares_outstanding"):
            if cur.get(fld) is None and prev.get(fld) is not None:
                cur[fld] = prev.get(fld)

        sector = DynamicSectorAverages.normalize_sector(fin.get("sector", "ãã®ä»–"))
        med = DynamicSectorAverages.SECTOR_MEDIANS.get(sector, DynamicSectorAverages.SECTOR_MEDIANS["ãã®ä»–"])
        ca_ratio = med.get("ca_ratio")
        cl_ratio = med.get("cl_ratio")
        gpm_med  = med.get("gpm")

        if (cur.get("current_assets") is None and cur.get("total_assets") and ca_ratio):
            cur["current_assets"] = cur["total_assets"] * ca_ratio

        if (cur.get("current_liabilities") is None and cur.get("total_assets") and cur.get("equity") and cl_ratio):
            cur["current_liabilities"] = (cur["total_assets"] - cur["equity"]) * cl_ratio

        if cur.get("gross_profit_margin") is None and gpm_med:
            cur["gross_profit_margin"] = gpm_med * 0.95

        fin["current"] = cur
        fin["previous"] = prev
        for k, v in cur.items():
            fin[f"current_{k}"] = v
        return fin

# ------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ------------------------------------------------------------
def enhance_stock_list_with_sectors(df: pd.DataFrame) -> pd.DataFrame:
    if "Code" not in df.columns:
        return df
    if "Sector33Name" not in df.columns:
        df["Sector33Name"] = df["Code"].astype(str).map(DynamicSectorAverages.get_sector_static).fillna("ãã®ä»–")
    if "MarketCode" not in df.columns:
        df["MarketCode"] = ""
    if "CompanyName" not in df.columns:
        df["CompanyName"] = ""
    return df[["Code", "CompanyName", "Sector33Name", "MarketCode"]]

# ------------------------------------------------------------
# éŠ˜æŸ„åãƒ•ã‚£ãƒ«ã‚¿
# ------------------------------------------------------------
def check_company_name_validity(company_name: str) -> Tuple[bool, str]:
    if not company_name:
        return True, "OK"
    etf_keywords = [
        'ï¼¥ï¼´ï¼¦','ETF','ä¸Šå ´æŠ•ä¿¡','ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ãƒ³ãƒ‰','é€£å‹•å‹ä¸Šå ´æŠ•ä¿¡',
        'ä¸Šå ´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹','TOPIX','æ—¥çµŒ225','æŠ•è³‡æ³•äºº','ãƒªãƒ¼ãƒˆ','REIT'
    ]
    if any(k in company_name for k in etf_keywords):
        return False, "ETF/æŠ•ä¿¡"
    fund_company_keywords = ['ã‚¢ã‚»ãƒƒãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ','æŠ•ä¿¡']
    if any(k in company_name for k in fund_company_keywords):
        return False, "æŠ•ä¿¡ä¼šç¤¾å•†å“"
    return True, "OK"

# ------------------------------------------------------------
# ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«
# ------------------------------------------------------------
def calculate_rsi(prices: pd.Series, period: int = 14) -> float:
    if len(prices) < period + 1:
        return 50.0
    delta = prices.diff()
    gain = delta.clip(lower=0).rolling(window=period).mean()
    loss = (-delta.clip(upper=0)).rolling(window=period).mean()
    rs = gain / loss.replace(0, np.nan)
    rsi = 100 - (100 / (1 + rs))
    v = rsi.iloc[-1]
    return float(v) if np.isfinite(v) else 50.0

def calculate_adx_and_di(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> Tuple[float, float, float]:
    if min(len(high), len(low), len(close)) < period + 5:
        return 20.0, 20.0, 20.0

    df = pd.DataFrame({"high": high, "low": low, "close": close}).dropna()
    if len(df) < period + 1:
        return 20.0, 20.0, 20.0

    high, low, close = df["high"], df["low"], df["close"]
    tr = pd.concat([
        (high - low),
        (high - close.shift(1)).abs(),
        (low - close.shift(1)).abs(),
    ], axis=1).max(axis=1)

    up_move = high.diff()
    down_move = -low.diff()

    plus_dm = up_move.where((up_move > 0) & (up_move > down_move), 0.0)
    minus_dm = down_move.where((down_move > 0) & (down_move > up_move), 0.0)

    atr = tr.ewm(span=period, adjust=False).mean()
    plus_di  = 100 * (plus_dm.ewm(span=period, adjust=False).mean() / atr.replace(0, np.nan))
    minus_di = 100 * (minus_dm.ewm(span=period, adjust=False).mean() / atr.replace(0, np.nan))
    dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di)
    adx = dx.ewm(span=period, adjust=False).mean()

    def clamp(x, lo, hi):
        return float(max(lo, min(hi, x))) if np.isfinite(x) else float(lo)

    return clamp(adx.iloc[-1], 5, 80), clamp(plus_di.iloc[-1], 5, 95), clamp(minus_di.iloc[-1], 5, 95)

def calculate_moving_averages(prices: pd.Series, periods: Tuple[int, ...] = (25, 75, 200)) -> Dict[str, float]:
    out: Dict[str, float] = {}
    for p in periods:
        if len(prices) >= p:
            ma = prices.rolling(window=p).mean().iloc[-1]
            out[f"ma_{p}"] = float(ma) if np.isfinite(ma) else float(prices.iloc[-1])
        elif len(prices):
            out[f"ma_{p}"] = float(prices.iloc[-1])
        else:
            out[f"ma_{p}"] = None
    return out

def calculate_volatility(prices: pd.Series, period: int = 20) -> Tuple[Optional[float], Optional[float]]:
    if len(prices) < max(5, period):
        return None, None
    try:
        returns = prices.pct_change(fill_method=None).dropna()
    except TypeError:
        returns = prices.pct_change().dropna()
    cur = returns.tail(period).std() * np.sqrt(252) if len(returns) >= period else returns.std() * np.sqrt(252)
    avg = returns.std() * np.sqrt(252)
    return float(cur), float(avg)

# ------------------------------------------------------------
# é•·æœŸæŠ•è³‡å‘ã‘: æœ€å¤§DD / å£²ä¸ŠCAGR / å®‰å…¨åŸºæº–
# ------------------------------------------------------------
def calculate_max_drawdown(prices: pd.Series, lookback_days: Optional[int] = None) -> Optional[float]:
    if prices is None or len(prices) < 2:
        return None
    try:
        prices_series = prices.copy()
        if lookback_days is not None and lookback_days > 1:
            prices_series = prices_series.tail(lookback_days)
        if len(prices_series) < 2:
            return None

        prices_sorted = prices_series.sort_index() if hasattr(prices_series.index, "is_monotonic_increasing") else prices_series
        cumulative_max = prices_sorted.cummax()
        drawdowns = (prices_sorted - cumulative_max) / cumulative_max
        max_dd = float(drawdowns.min())
        return max_dd if np.isfinite(max_dd) else None
    except Exception:
        return None

def _pick_numeric_field(record: dict, keys: List[str]) -> Optional[float]:
    for key in keys:
        if key in record and record[key] not in (None, "", "NA"):
            try:
                return float(record[key])
            except Exception:
                continue
    return None

def _fiscal_year_from_statement(record: dict) -> int:
    for ky in ("fiscalYear", "FiscalYear", "period", "CurrentFiscalYearEndDate", "DisclosedDate"):
        value = str(record.get(ky) or "")
        match = re.findall(r"\d{4}", value)
        if match:
            return int(match[0])
    return -1

def build_financial_history_from_statements(stmts: List[dict], max_years: int = 5) -> List[dict]:
    if not stmts:
        return []
    sorted_stmts = sorted(stmts, key=_fiscal_year_from_statement, reverse=True)
    history: list[dict] = []
    for stmt in sorted_stmts:
        rec = {
            "fiscal_year": _fiscal_year_from_statement(stmt),
            "revenue": _pick_numeric_field(stmt, ["NetSales", "Revenue", "OperatingRevenue"]),
            "operating_income": _pick_numeric_field(stmt, ["OperatingIncome", "OperatingIncomeLoss", "OperatingProfit"]),
            "net_income": _pick_numeric_field(stmt, ["NetIncomeLoss", "Profit", "ProfitAttributableToOwnersOfParent", "NetIncome"]),
            "operating_cash_flow": _pick_numeric_field(stmt, ["NetCashProvidedByUsedInOperatingActivities", "CashFlowsFromOperatingActivities"]),
            "total_assets": _pick_numeric_field(stmt, ["TotalAssets"]),
            "equity": _pick_numeric_field(stmt, ["EquityAttributableToOwnersOfParent", "Equity", "NetAssets"]),
            "current_assets": _pick_numeric_field(stmt, ["CurrentAssets"]),
            "current_liabilities": _pick_numeric_field(stmt, ["CurrentLiabilities"]),
            "gross_profit_margin": None,
            "shares_outstanding": _pick_numeric_field(
                stmt,
                [
                    "NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock",
                    "NumberOfIssuedAndOutstandingShares",
                ],
            ),
        }

        cash_and_equivalents = _pick_numeric_field(
            stmt,
            [
                "CashAndCashEquivalents",
                "CashAndCashEquivalentsAtEndOfPeriod",
                "Cash",
                "CashEquivalents",
                "CashAndDeposits",
            ],
        )
        if cash_and_equivalents is None:
            cash_and_equivalents = rec["current_assets"]
        rec["cash_and_equivalents"] = cash_and_equivalents

        if rec["total_assets"] is not None and rec["total_assets"] > 0 and rec["equity"] is not None:
            rec["equity_ratio"] = rec["equity"] / rec["total_assets"]
        else:
            rec["equity_ratio"] = None

        gross_profit = _pick_numeric_field(stmt, ["GrossProfit"])
        if rec["revenue"] and gross_profit and rec["revenue"] != 0:
            rec["gross_profit_margin"] = gross_profit / rec["revenue"]

        history.append(rec)
        if len(history) >= max_years:
            break
    return history

def compute_sales_cagr(history: List[dict], years: int = 3) -> Optional[float]:
    if not history or len(history) <= years:
        return None
    latest = history[0].get("revenue")
    past = history[years].get("revenue") if len(history) > years else None
    if not latest or not past or past <= 0 or latest <= 0:
        return None
    try:
        return (latest / past) ** (1 / years) - 1
    except (ZeroDivisionError, OverflowError):
        return None

def evaluate_operating_income_stability(history: List[dict],
                                        years: int = OP_INCOME_YEARS,
                                        drop_floor: float = OP_INCOME_DROP_FLOOR,
                                        exclude_deficit: bool = EXCLUDE_OP_INCOME_DEFICIT) -> dict:
    out = {"stable": None, "reason": "insufficient", "years_checked": years, "latest": None, "median_prev": None}
    if not history:
        return out

    op: list[Optional[float]] = []
    for rec in history[:max(1, years)]:
        v = rec.get("operating_income")
        op.append(None if v is None else float(v))

    if len(op) < 1 or op[0] is None:
        out["reason"] = "latest_missing"
        return out

    out["latest"] = op[0]

    if any(v is None for v in op):
        out["reason"] = "missing_in_window"
        out["stable"] = None
        return out

    if exclude_deficit and any(v <= 0 for v in op):
        out["stable"] = False
        out["reason"] = "deficit_in_window"
        return out

    prev = op[1:] if len(op) > 1 else []
    if prev:
        med_prev = float(np.median(prev))
        out["median_prev"] = med_prev
        if med_prev > 0 and op[0] < med_prev * float(drop_floor):
            out["stable"] = False
            out["reason"] = f"drop_below_floor({drop_floor})"
            return out

    out["stable"] = True
    out["reason"] = "ok"
    return out

def calculate_safety_criteria_v1(
    ps_ratio: Optional[float],
    cash_and_equivalents: Optional[float],
    market_cap: Optional[float],
    operating_cash_flow: Optional[float],
    equity_ratio: Optional[float],
    sales_cagr: Optional[float],
    max_drawdown: Optional[float],
) -> dict:
    criteria = {
        "ps_under_1": False,
        "cash_rich": False,
        "positive_ocf": False,
        "equity_ratio_50plus": False,
        "equity_ratio_70plus": False,
        "growth_potential": False,
        "no_speculative_drop": False,
    }
    scores: dict[str, float] = {}
    total_score = 0.0
    max_score = 100.0

    if ps_ratio is not None and ps_ratio < 1.0:
        criteria["ps_under_1"] = True
        scores["ps_under_1"] = 25.0
        total_score += 25.0
    else:
        scores["ps_under_1"] = 0.0

    if (cash_and_equivalents is not None and market_cap is not None and market_cap > 0 and cash_and_equivalents > market_cap):
        criteria["cash_rich"] = True
        scores["cash_rich"] = 20.0
        total_score += 20.0
    else:
        scores["cash_rich"] = 0.0

    if operating_cash_flow is not None and operating_cash_flow > 0:
        criteria["positive_ocf"] = True
        scores["positive_ocf"] = 20.0
        total_score += 20.0
    else:
        scores["positive_ocf"] = 0.0

    if equity_ratio is not None and equity_ratio >= 0.5:
        criteria["equity_ratio_50plus"] = True
        scores["equity_ratio_50plus"] = 15.0
        total_score += 15.0
    else:
        scores["equity_ratio_50plus"] = 0.0

    if equity_ratio is not None and equity_ratio >= 0.7:
        criteria["equity_ratio_70plus"] = True
        scores["equity_ratio_70plus"] = 10.0
        total_score += 10.0
    else:
        scores["equity_ratio_70plus"] = 0.0

    if sales_cagr is not None and sales_cagr > 0:
        criteria["growth_potential"] = True
        scores["growth_potential"] = 5.0
        total_score += 5.0
    else:
        scores["growth_potential"] = 0.0

    if max_drawdown is not None:
        if max_drawdown > -0.8:
            criteria["no_speculative_drop"] = True
            scores["no_speculative_drop"] = 5.0
            total_score += 5.0
        else:
            scores["no_speculative_drop"] = 0.0
    else:
        scores["no_speculative_drop"] = 2.5
        total_score += 2.5

    required_conditions_met = (
        criteria["ps_under_1"] and
        criteria["positive_ocf"] and
        criteria["equity_ratio_50plus"] and
        criteria["no_speculative_drop"]
    )

    return {
        "criteria": criteria,
        "scores": scores,
        "total_score": round(total_score, 1),
        "max_score": max_score,
        "required_conditions_met": required_conditions_met,
        "ps_ratio": ps_ratio,
        "cash_and_equivalents": cash_and_equivalents,
        "equity_ratio": equity_ratio,
        "sales_cagr": sales_cagr,
        "max_drawdown": max_drawdown,
    }

# ------------------------------------------------------------
# Piotroskiï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
# ------------------------------------------------------------
def calculate_piotroski_real(fin: dict) -> dict:
    def nz(x, default=0.0):
        if x in (None, "", "NA") or (isinstance(x, float) and (np.isnan(x) or not np.isfinite(x))):
            return default
        return float(x)
    def safe_ratio(a, b):
        a, b = nz(a, 0.0), nz(b, 0.0)
        return a / b if b != 0 else 0.0

    cur  = {k: nz(v, None) for k, v in fin.get("current", {}).items()}
    prev = {k: nz(v, None) for k, v in fin.get("previous", {}).items()}

    comp = {}
    comp["positive_net_income"] = nz(cur.get("net_income")) > 0
    comp["positive_ocf"] = nz(cur.get("operating_cash_flow")) > 0
    comp["ocf_gt_ni"] = nz(cur.get("operating_cash_flow")) > nz(cur.get("net_income"))
    comp["roa_up"] = safe_ratio(cur.get("net_income"), cur.get("total_assets")) > safe_ratio(prev.get("net_income"), prev.get("total_assets"))
    comp["ocf_margin_up"] = safe_ratio(cur.get("operating_cash_flow"), cur.get("revenue")) > safe_ratio(prev.get("operating_cash_flow"), prev.get("revenue"))
    comp["current_ratio_up"] = safe_ratio(cur.get("current_assets"), cur.get("current_liabilities")) > safe_ratio(prev.get("current_assets"), prev.get("current_liabilities"))
    comp["shares_down"] = nz(cur.get("shares_outstanding")) < nz(prev.get("shares_outstanding"))
    comp["gpm_up"] = nz(cur.get("gross_profit_margin")) > nz(prev.get("gross_profit_margin"))
    lev_cur  = safe_ratio(nz(cur.get("total_assets")) - nz(cur.get("equity")), nz(cur.get("total_assets")))
    lev_prev = safe_ratio(nz(prev.get("total_assets")) - nz(prev.get("equity")), nz(prev.get("total_assets")))
    comp["leverage_down"] = lev_cur < lev_prev

    score = int(sum(bool(v) for v in comp.values()))
    evaluation = ("å„ªç§€" if score >= 7 else "è‰¯å¥½" if score >= 5 else "æ™®é€š" if score >= 3 else "æ³¨æ„")
    return {"score": score, "details": comp, "evaluation": evaluation, "mode": "real"}

# ------------------------------------------------------------
# ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ¢ãƒƒã‚¯ãªã—ï¼‰
# ------------------------------------------------------------
def calculate_ps_ratio(current_price: Optional[float], revenue_per_share: Optional[float]=None,
                       market_cap: Optional[float]=None, revenue: Optional[float]=None) -> Optional[float]:
    try:
        if current_price and revenue_per_share and revenue_per_share > 0:
            return float(current_price) / float(revenue_per_share)
        if market_cap and revenue and revenue > 0:
            return float(market_cap) / float(revenue)
        return None
    except Exception:
        return None

def calculate_peg_ratio(per: Optional[float], eps_growth_rate_pct: Optional[float]) -> Optional[float]:
    """PEG=PER/EPSæˆé•·ç‡ã€‚0ã‚„æ¥µå°å€¤ã¯æ¬ ææ‰±ã„ï¼ˆæˆé•·ç‡ç•°å¸¸ãƒ»ãƒ‡ãƒ¼ã‚¿ä¸å‚™ã®å¯èƒ½æ€§ï¼‰"""
    try:
        if per is None or eps_growth_rate_pct is None:
            return None
        if per <= 0 or eps_growth_rate_pct <= 0:
            return None
        peg = float(per) / float(eps_growth_rate_pct)
        if peg <= 0 or not np.isfinite(peg):
            return None
        if peg < 0.01:
            return None
        return peg
    except Exception:
        return None

def estimate_eps_growth_rate(net_income_current: Optional[float],
                             net_income_previous: Optional[float],
                             shares_outstanding: Optional[float]) -> Optional[float]:
    try:
        if not all(v is not None for v in [net_income_current, net_income_previous, shares_outstanding]):
            return None
        if shares_outstanding <= 0:
            return None
        eps_cur = float(net_income_current) / float(shares_outstanding)
        eps_prev = float(net_income_previous) / float(shares_outstanding)
        if eps_prev <= 0:
            return None
        return (eps_cur / eps_prev - 1.0) * 100.0
    except Exception:
        return None

def calculate_valuation_metrics_ps_peg(current_price: Optional[float],
                                       net_income_current: Optional[float],
                                       net_income_previous: Optional[float],
                                       revenue_current: Optional[float],
                                       shares_outstanding: Optional[float]) -> dict:
    rps = None
    if revenue_current and shares_outstanding and shares_outstanding > 0:
        rps = revenue_current / shares_outstanding

    per = None
    if net_income_current and shares_outstanding and shares_outstanding > 0:
        eps = net_income_current / shares_outstanding
        if eps > 0 and current_price and current_price > 0:
            per = current_price / eps

    eps_growth = estimate_eps_growth_rate(net_income_current, net_income_previous, shares_outstanding)
    ps_ratio = calculate_ps_ratio(current_price, revenue_per_share=rps)
    peg_ratio = calculate_peg_ratio(per, eps_growth)
    return {
        "revenue_per_share": rps,
        "per": per,
        "ps_ratio": ps_ratio,
        "eps_growth_rate": eps_growth,
        "peg_ratio": peg_ratio,
        "peg_trusted": (peg_ratio is not None)
    }

# ------------------------------------------------------------
# å®‰å…¨æ€§ãƒ»æŠ•æ©Ÿæ€§
# ------------------------------------------------------------
def calculate_safety_score_v3(
    margin_ratio: float = None,
    short_selling_change_rate: float = None,
    yoy_eps_growth: float = None,
    dividend_status: str = None,
    avg_volume: int = None,
    stagnant_days_after_spike: int = None,
    current_volatility: float = None,
    average_volatility: float = None,
    below_ma25: bool = False,
    below_ma75: bool = False
) -> dict:
    safety_score = 0.0
    details = {}
    max_total_score = 25.0
    w = {'margin_ratio':4.0,'short_selling':4.0,'earnings_stability':3.5,'dividend_stability':3.0,
         'liquidity':2.5,'momentum_stability':2.5,'volatility_stability':2.5,'technical_strength':3.0}

    margin_score = w['margin_ratio'] * (0.6 if margin_ratio is None else 1.0 if margin_ratio<=3 else 0.8 if margin_ratio<=5 else 0.6 if margin_ratio<=10 else 0.3 if margin_ratio<=20 else 0)
    short_score  = w['short_selling'] * (0.6 if short_selling_change_rate is None else 1.0 if short_selling_change_rate<=5 else 0.8 if short_selling_change_rate<=15 else 0.5 if short_selling_change_rate<=30 else 0.2 if short_selling_change_rate<=50 else 0)
    safety_score += margin_score + short_score
    details['ä¿¡ç”¨å®‰å…¨æ€§'] = f"{'ä¸æ˜' if margin_ratio is None else f'{margin_ratio:.1f}å€'} ({margin_score:.1f})"
    details['ç©ºå£²ã‚Šå®‰å…¨æ€§'] = f"{'ä¸æ˜' if short_selling_change_rate is None else f'{short_selling_change_rate:.1f}%'} ({short_score:.1f})"

    eps_score = w['earnings_stability'] * (0.5 if yoy_eps_growth is None else 1.0 if yoy_eps_growth>=20 else 0.8 if yoy_eps_growth>=10 else 0.7 if yoy_eps_growth>=0 else 0.4 if yoy_eps_growth>=-10 else 0.2 if yoy_eps_growth>=-20 else 0)
    div_score = w['dividend_stability'] * (0.5 if not dividend_status else 1.0 if dividend_status=='å¢—é…' else 0.8 if dividend_status=='ç¶­æŒ' else 0.3 if dividend_status=='æœªå®š' else 0.1 if dividend_status=='æ¸›é…' else 0)
    safety_score += eps_score + div_score
    details['æ¥­ç¸¾å®‰å®šæ€§'] = f"{'ä¸æ˜' if yoy_eps_growth is None else f'EPSæˆé•·ç‡{yoy_eps_growth:.1f}%'} ({eps_score:.1f})"
    details['é…å½“å®‰å®šæ€§'] = f"{dividend_status or 'ä¸æ˜'} ({div_score:.1f})"

    volume_score = w['liquidity'] * (0.5 if avg_volume is None else 1.0 if avg_volume>=500000 else 0.8 if avg_volume>=200000 else 0.6 if avg_volume>=100000 else 0.3 if avg_volume>=50000 else 0)
    safety_score += volume_score
    details['æµå‹•æ€§'] = f"{'ä¸æ˜' if avg_volume is None else f'{avg_volume:,}æ ª'} ({volume_score:.1f})"

    stagnant_score = w['momentum_stability'] * (0.6 if stagnant_days_after_spike is None else 1.0 if stagnant_days_after_spike==0 else 0.8 if stagnant_days_after_spike<=2 else 0.5 if stagnant_days_after_spike<=4 else 0.2 if stagnant_days_after_spike<=6 else 0)
    if current_volatility is not None and average_volatility not in (None, 0):
        vr = current_volatility / average_volatility
        vol_score = w['volatility_stability'] * (1.0 if vr<=1.2 else 0.8 if vr<=1.5 else 0.5 if vr<=2.0 else 0.2 if vr<=2.5 else 0)
        vol_note = f"{vr:.1f}å€"
    else:
        vol_score = w['volatility_stability'] * 0.6
        vol_note = "ä¸æ˜"
    safety_score += stagnant_score + vol_score
    details['ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ å®‰å®šæ€§'] = f"{'ä¸æ˜' if stagnant_days_after_spike is None else f'{stagnant_days_after_spike}æ—¥'} ({stagnant_score:.1f})"
    details['ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å®‰å®šæ€§'] = f"{vol_note} ({vol_score:.1f})"

    if not below_ma25 and not below_ma75:
        tech_score = w['technical_strength']
        tech_note = "25æ—¥ãƒ»75æ—¥ç·šä¸Šæ–¹"
    elif not below_ma25 or not below_ma75:
        tech_score = w['technical_strength'] * 0.5
        tech_note = "ä¸€éƒ¨ç§»å‹•å¹³å‡ç·šä¸Šæ–¹"
    else:
        tech_score = 0.0
        tech_note = "25æ—¥ãƒ»75æ—¥ç·šä¸‹æ–¹"
    safety_score += tech_score
    details['ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«å¼·ã•'] = f"{tech_note} ({tech_score:.1f})"

    ratio = safety_score / max_total_score
    level = "ğŸŸ¢ éå¸¸ã«å®‰å…¨" if ratio>=0.8 else "ğŸ”µ å®‰å…¨" if ratio>=0.6 else "ğŸŸ¡ æ™®é€š" if ratio>=0.4 else "ğŸŸ  ã‚„ã‚„å±é™º" if ratio>=0.2 else "ğŸ”´ å±é™º"
    return {"total_score": round(safety_score,1), "max_score": max_total_score, "safety_level": level, "details": details}

def detect_speculative_manipulation_v2(
    margin_ratio: float | None = None,
    short_selling_change_rate: float | None = None,
    yoy_eps_growth: float | None = None,
    dividend_status: str | None = None,
    avg_volume: int | None = None,
    stagnant_days_after_spike: int | None = None,
    current_volatility: float | None = None,
    average_volatility: float | None = None,
    below_ma25: bool = False,
    below_ma75: bool = False,
    current_price: float | None = 1000.0,
    mas: dict | None = None,
    stock_code: str | None = None
) -> dict:
    score = 0
    flags = []; risks = []
    if margin_ratio is not None:
        if margin_ratio >= 50: score += 25; flags.append(f"ğŸš¨ ä¿¡ç”¨å€ç‡ç•°å¸¸é«˜: {margin_ratio:.1f}å€")
        elif margin_ratio >= 20: score += 15; flags.append(f"âš ï¸ ä¿¡ç”¨å€ç‡é«˜: {margin_ratio:.1f}å€")
        elif margin_ratio >= 10: score += 8; risks.append(f"ä¿¡ç”¨å€ç‡ã‚„ã‚„é«˜: {margin_ratio:.1f}å€")
    if short_selling_change_rate is not None:
        if short_selling_change_rate >= 100: score += 20; flags.append(f"ğŸš¨ ç©ºå£²ã‚Šæ®‹æ€¥å¢—: +{short_selling_change_rate:.1f}%")
        elif short_selling_change_rate >= 50: score += 12; flags.append(f"âš ï¸ ç©ºå£²ã‚Šæ®‹å¢—åŠ : +{short_selling_change_rate:.1f}%")
        elif short_selling_change_rate >= 25: score += 6; risks.append(f"ç©ºå£²ã‚Šæ®‹ã‚„ã‚„å¢—åŠ : +{short_selling_change_rate:.1f}%")
    if stagnant_days_after_spike is not None:
        if stagnant_days_after_spike >= 5: score += 15; flags.append(f"ğŸ“‰ æ€¥é¨°å¾Œã®æ¨ªã°ã„: {stagnant_days_after_spike}æ—¥")
        elif stagnant_days_after_spike >= 3: score += 8; risks.append(f"æ¨ªã°ã„å‚¾å‘: {stagnant_days_after_spike}æ—¥")
    if current_volatility is not None and average_volatility not in (None, 0):
        vr = current_volatility / average_volatility
        if vr >= 3.0: score += 20; flags.append(f"ğŸš¨ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç•°å¸¸: {vr:.1f}å€")
        elif vr >= 2.0: score += 12; flags.append(f"âš ï¸ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é«˜: {vr:.1f}å€")
        elif vr >= 1.5: score += 6; risks.append(f"ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚„ã‚„é«˜: {vr:.1f}å€")
    if below_ma25 and below_ma75: score += 8; flags.append("âš ï¸ 25ãƒ»75æ—¥ç·šã®ä¸¡æ–¹å‰²ã‚Œ")
    elif below_ma25 or below_ma75: score += 4; risks.append("ç§»å‹•å¹³å‡ç·šã®ä¸€éƒ¨å‰²ã‚Œ")
    if avg_volume is not None and avg_volume < 30000: score += 8; flags.append(f"âš ï¸ æµå‹•æ€§ä½: {avg_volume:,}æ ª/æ—¥")
    if dividend_status in {"æœªå®š","æ¸›é…"}: score += 6; risks.append(f"é…å½“{dividend_status}")
    if yoy_eps_growth is not None and yoy_eps_growth < -30: score += 8; flags.append(f"âš ï¸ EPSæ€¥æ¸›: {yoy_eps_growth:.1f}%")

    level = "ğŸ”´ æ¥µã‚ã¦æŠ•æ©Ÿçš„" if score>=70 else "ğŸŸ  é«˜ã„" if score>=50 else "ğŸŸ¡ ã‚„ã‚„é«˜ã„" if score>=30 else "ğŸŸ¢ ä½ã„"
    return {"score": score, "level": level, "warning_flags": flags, "risk_factors": risks, "max_score": 100}

# ------------------------------------------------------------
# å˜éŠ˜æŸ„ãƒ•ãƒ«åˆ†æï¼ˆofflineå¯¾å¿œã€ãƒ¢ãƒƒã‚¯ãªã—ï¼‰
# ------------------------------------------------------------
def analyze_single_stock_complete_v3(session: requests.Session,
                                     sector_averages: dict,
                                     code: str,
                                     name: str = "",
                                     market: str = "",
                                     sector_hint: str | None = None,
                                     *,
                                     offline: bool = False) -> dict:
    try:
        fdm = FinancialDataManager(session)
        sector_raw = sector_hint or DynamicSectorAverages.get_sector_static(code)
        sector = DynamicSectorAverages.normalize_sector(sector_raw)
        fc = FrozenCache()

        # ä¾¡æ ¼
        if offline:
            price_df = fc.load_prices(code)
        else:
            price_df = fdm.load_or_download_data_v2(build_prices_endpoint(code), f"prices_{code}")
        if price_df is None or price_df.empty:
            return {"stock_code": code, "company_name": name, "sector_name": sector, "success": False, "error": "price_missing"}

        def _col(df, *cands):
            lc = {c.lower(): c for c in df.columns}
            for c in cands:
                for k, v in lc.items():
                    if k == c.lower():
                        return v
            return None

        c_close = _col(price_df, "Close","ClosePrice","EndPrice","AdjustmentClose","AdjClose")
        c_high  = _col(price_df, "High","HighPrice")
        c_low   = _col(price_df, "Low","LowPrice")
        c_vol   = _col(price_df, "Volume","TradingVolume")
        c_date  = _col(price_df, "Date","TradingDate")
        if c_date:
            price_df = price_df.sort_values(c_date)

        close = price_df[c_close].astype(float) if c_close in price_df.columns else pd.Series([], dtype=float)
        high  = price_df[c_high].astype(float)  if c_high  in price_df.columns else close
        low   = price_df[c_low].astype(float)   if c_low   in price_df.columns else close
        vol_s = price_df[c_vol].astype(float)   if c_vol   in price_df.columns else None

        current_price = float(close.iloc[-1]) if len(close) else None
        mas = calculate_moving_averages(close) if len(close) else {}
        rsi = float(calculate_rsi(close)) if len(close) else None
        adx, plus_di, minus_di = calculate_adx_and_di(high, low, close) if len(close) else (None, None, None)
        cur_vol, avg_vol = calculate_volatility(close) if len(close) else (None, None)

        below_ma25 = bool(current_price is not None and mas.get("ma_25") is not None and current_price < mas["ma_25"])
        below_ma75 = bool(current_price is not None and mas.get("ma_75") is not None and current_price < mas["ma_75"])
        below_ma200 = bool(current_price is not None and mas.get("ma_200") is not None and current_price < mas["ma_200"])
        avg_volume = int(vol_s.tail(30).mean()) if isinstance(vol_s, pd.Series) and len(vol_s) else None

        # è²¡å‹™
        if offline:
            stmts = fc.load_statements(code)
        else:
            stmts = fdm.fetch_statements(code)

        financial_history = build_financial_history_from_statements(stmts if isinstance(stmts, list) else [], max_years=5)
        cur_fin = financial_history[0].copy() if financial_history else {}
        prv_fin = financial_history[1].copy() if len(financial_history) > 1 else {}

        fin = {"current": cur_fin, "previous": prv_fin, "current_price": current_price, "sector": sector}
        fin = fdm._fill_missing_fields(fin)

        # æŒ‡æ¨™
        piot = calculate_piotroski_real(fin)
        val = calculate_valuation_metrics_ps_peg(
            current_price=current_price,
            net_income_current=fin["current"].get("net_income"),
            net_income_previous=fin["previous"].get("net_income"),
            revenue_current=fin["current"].get("revenue"),
            shares_outstanding=fin["current"].get("shares_outstanding"),
        )
        safety = calculate_safety_score_v3(
            yoy_eps_growth=val.get("eps_growth_rate"),
            avg_volume=avg_volume,
            current_volatility=cur_vol, average_volatility=avg_vol,
            below_ma25=below_ma25, below_ma75=below_ma75
        )
        spec = detect_speculative_manipulation_v2(
            yoy_eps_growth=val.get("eps_growth_rate"),
            avg_volume=avg_volume,
            current_volatility=cur_vol, average_volatility=avg_vol,
            below_ma25=below_ma25, below_ma75=below_ma75,
            current_price=current_price, mas=mas, stock_code=code
        )

        shares_outstanding = fin["current"].get("shares_outstanding")
        market_cap = None
        if current_price is not None and shares_outstanding not in (None, 0):
            market_cap = current_price * shares_outstanding

        max_dd = calculate_max_drawdown(close, lookback_days=LOOKBACK_DAYS) if len(close) > 0 else None
        sales_cagr = compute_sales_cagr(financial_history, years=3) if financial_history else None
        cash_eq = fin["current"].get("cash_and_equivalents")
        equity_ratio = fin["current"].get("equity_ratio")
        if equity_ratio is None:
            ta = fin["current"].get("total_assets")
            eq = fin["current"].get("equity")
            if ta not in (None, 0) and eq is not None:
                equity_ratio = eq / ta

        safety_criteria = calculate_safety_criteria_v1(
            ps_ratio=val.get("ps_ratio"),
            cash_and_equivalents=cash_eq,
            market_cap=market_cap,
            operating_cash_flow=fin["current"].get("operating_cash_flow"),
            equity_ratio=equity_ratio,
            sales_cagr=sales_cagr,
            max_drawdown=max_dd,
        )

        # â˜…å¿…é ˆãƒ•ã‚£ãƒ«ã‚¿
        liquidity_ok = (avg_volume is not None and avg_volume >= MIN_AVG_VOLUME_30D)
        market_cap_ok = (market_cap is not None and market_cap >= MIN_MARKET_CAP_JPY)

        ps_ratio = val.get("ps_ratio")
        per = val.get("per")

        defensive_ps_ok = (ps_ratio is not None and np.isfinite(ps_ratio) and ps_ratio <= MAX_PS_DEFENSIVE)
        per_satellite = (per is not None and np.isfinite(per) and per > MAX_PER_CORE)
        per_core_ok = (per is not None and np.isfinite(per) and per <= MAX_PER_CORE)

        op_income_eval = evaluate_operating_income_stability(
            financial_history,
            years=OP_INCOME_YEARS,
            drop_floor=OP_INCOME_DROP_FLOOR,
            exclude_deficit=EXCLUDE_OP_INCOME_DEFICIT
        )
        op_income_stable = (op_income_eval.get("stable") is True)

        base_ok = bool(liquidity_ok and market_cap_ok and op_income_stable)
        core_candidate = bool(base_ok and defensive_ps_ok and per_core_ok)
        satellite_candidate = bool(base_ok and (not core_candidate))

        filter_details = {
            "liquidity_ok": liquidity_ok,
            "market_cap_ok": market_cap_ok,
            "defensive_ps_ok": defensive_ps_ok,
            "per_satellite": per_satellite,
            "per_core_ok": per_core_ok,
            "op_income_stable": op_income_stable,
            "op_income_reason": op_income_eval.get("reason"),
            "base_ok": base_ok,
            "core_candidate": core_candidate,
            "satellite_candidate": satellite_candidate,
            "thresholds": {
                "MIN_AVG_VOLUME_30D": MIN_AVG_VOLUME_30D,
                "MIN_MARKET_CAP_JPY": MIN_MARKET_CAP_JPY,
                "MAX_PS_DEFENSIVE": MAX_PS_DEFENSIVE,
                "MAX_PER_CORE": MAX_PER_CORE,
                "OP_INCOME_YEARS": OP_INCOME_YEARS,
                "OP_INCOME_DROP_FLOOR": OP_INCOME_DROP_FLOOR,
                "EXCLUDE_OP_INCOME_DEFICIT": EXCLUDE_OP_INCOME_DEFICIT,
            }
        }

        return {
            "stock_code": code, "company_name": name, "sector_name": sector,
            "current_price": current_price, "mas": mas, "rsi": rsi, "adx": adx,
            "plus_di": plus_di, "minus_di": minus_di,
            "volatility": cur_vol, "avg_volatility": avg_vol,
            "below_ma25": below_ma25, "below_ma75": below_ma75, "below_ma200": below_ma200,
            "piotroski": piot,
            "ps_ratio": ps_ratio, "peg_ratio": val.get("peg_ratio"), "per": per,
            "revenue_per_share": val.get("revenue_per_share"),
            "safety": safety, "speculation": spec, "success": True,
            "avg_volume_30d": avg_volume,
            "financial_history": financial_history,
            "market_cap": market_cap,
            "shares_outstanding": shares_outstanding,
            "max_drawdown": max_dd,
            "sales_cagr": sales_cagr,
            "safety_criteria": safety_criteria,
            "filters": filter_details,
        }
    except Exception as e:
        return {"stock_code": code, "company_name": name, "sector_name": sector_hint or "ãã®ä»–", "error": f"{e}", "success": False}

# ------------------------------------------------------------
# åé›†ï¼ˆå˜ä½“/å…¨ä½“ï¼‰
# ------------------------------------------------------------
def collect_one_code(session: requests.Session, code: str, name: str = "", *, force_refresh: bool = False) -> bool:
    fc = FrozenCache()
    helper = DynamicSectorAverages(session)
    try:
        # ä¾¡æ ¼
        price_df = helper.load_or_download_data_v2(
            build_prices_endpoint(code),
            f"prices_{code}",
            bypass_cache=force_refresh
        )
        if price_df is not None and not price_df.empty:
            fc.save_prices(code, price_df)

        # è²¡å‹™
        fdm = FinancialDataManager(session)
        stmts = fdm.fetch_statements(code, force_refresh=force_refresh)
        if stmts:
            fc.save_statements(code, stmts)

        return fc.has_all(code)
    except RuntimeError as e:
        if "æ—¥æ¬¡ãƒ¬ãƒ¼ãƒˆåˆ¶é™åˆ°é”" in str(e):
            raise
        return False
    except Exception:
        return False

PENDING_FILE = CACHE_DIR / "pending_codes.json"

def _save_pending(codes: list[str]) -> None:
    PENDING_FILE.write_text(json.dumps({"codes": codes}, ensure_ascii=False), encoding="utf-8")

def _load_pending(df: pd.DataFrame, *, force_full: bool = False, refresh_days: Optional[int] = None) -> list[str]:
    fc = FrozenCache()
    if force_full:
        codes = [str(c) for c in df["Code"].astype(str)]
        _save_pending(codes)
        return codes

    if refresh_days is not None:
        codes = [str(c) for c in df["Code"].astype(str) if not fc.has_all(str(c), max_age_days=refresh_days)]
        _save_pending(codes)
        return codes

    if PENDING_FILE.exists():
        try:
            return json.loads(PENDING_FILE.read_text(encoding="utf-8")).get("codes", [])
        except Exception:
            pass

    codes = [str(c) for c in df["Code"].astype(str) if not fc.has_all(str(c))]
    _save_pending(codes)
    return codes

def collect_all_daemon(session: requests.Session,
                       daily_budget: Optional[int] = None,
                       refresh_days: Optional[int] = None,
                       force_full: bool = False,
                       reset_pending: bool = False) -> None:
    fdm = FinancialDataManager(session)
    df = fdm.get_stock_list_v2(force_refresh=False)
    df = df[df.apply(lambda r: check_company_name_validity(str(r.get("CompanyName","")))[0], axis=1)].reset_index(drop=True)

    if reset_pending and PENDING_FILE.exists():
        try:
            PENDING_FILE.unlink()
        except Exception:
            pass

    pending = _load_pending(df, force_full=force_full, refresh_days=refresh_days)
    if not pending:
        print("ğŸ“¦ ã™ã§ã«å…¨ä»¶å–å¾—æ¸ˆã¿")
        return

    if daily_budget is None:
        rpd = int(os.getenv("JQ_RPD", "800"))
        daily_budget = max(1, min(len(pending), rpd // 2 - 5))

    mode = "å¼·åˆ¶å†åé›†" if force_full else (f"{refresh_days}æ—¥è¶…ã®ã¿å†åé›†" if refresh_days is not None else "æœªå–å¾—ã®ã¿")
    print(f"â–¶ å…¨è‡ªå‹•åé›†é–‹å§‹  æ®‹ã‚Š{len(pending)}éŠ˜æŸ„  æ—¥æ¬¡ä¸Šé™ç›®å®‰={daily_budget}éŠ˜æŸ„/æ—¥  ãƒ¢ãƒ¼ãƒ‰={mode}")

    while pending:
        taken = 0
        start = time.time()
        try:
            for code in list(pending):
                if taken >= daily_budget:
                    break
                ok = collect_one_code(session, code, force_refresh=(force_full or refresh_days is not None))
                if ok:
                    pending.remove(code)
                    _save_pending(pending)
                taken += 1
                if taken % 20 == 0 or taken == daily_budget:
                    elapsed = time.time() - start
                    print(f"  â± æœ¬æ—¥ {taken}/{daily_budget} ä»¶  æ®‹ã‚Š{len(pending)}  çµŒé{int(elapsed)}s", flush=True)
        except RuntimeError as e:
            if "æ—¥æ¬¡ãƒ¬ãƒ¼ãƒˆåˆ¶é™åˆ°é”" in str(e):
                pass
            else:
                raise

        print(f"ğŸ“¦ ä»Šæ—¥ã®åé›†ãƒãƒƒãƒçµ‚äº†: {taken}ä»¶  æ®‹ã‚Š{len(pending)}ä»¶")
        if not pending:
            print("âœ… å…¨éŠ˜æŸ„ã®å‡çµåé›†ãŒå®Œäº†")
            break

        wait_sec = seconds_until_next_day()
        h, rem = divmod(wait_sec, 3600)
        m, s = divmod(rem, 60)
        print(f"â³ æ—¥æ¬¡ä¸Šé™å›å¾©å¾…ã¡: {h}h{m}m{s}s å¾…æ©Ÿ")
        time.sleep(wait_sec)

def collect_batch(session: requests.Session, max_codes: int) -> dict:
    fdm = FinancialDataManager(session)
    df = fdm.get_stock_list_v2(force_refresh=False)
    df = df[df.apply(lambda r: check_company_name_validity(str(r.get("CompanyName","")))[0], axis=1)].reset_index(drop=True)
    fc = FrozenCache()

    pending = [str(c) for c in df["Code"].astype(str) if not fc.has_all(str(c))]
    picked  = pending[:max_codes]
    ok = 0
    fail = 0
    start = time.time()

    for i, code in enumerate(picked, 1):
        ok_flag = collect_one_code(session, code)
        if ok_flag:
            ok += 1
        else:
            fail += 1
        if i % 20 == 0 or i == len(picked):
            elapsed = time.time() - start
            print(f"  â± {i}/{len(picked)} åé›†ä¸­ (OK={ok} FAIL={fail}) çµŒé{elapsed:.0f}s", flush=True)

    return {"tried": len(picked), "ok": ok, "fail": fail}

# ------------------------------------------------------------
# ã‚ªãƒ•ãƒ©ã‚¤ãƒ³åˆ†æã‚¿ã‚¹ã‚¯ç”Ÿæˆ / éŠ˜æŸ„åå–å¾—
# ------------------------------------------------------------
def build_offline_analysis_tasks(session: requests.Session) -> list[tuple[str, str, str, str | None]]:
    fdm = FinancialDataManager(session)
    df_list = fdm.get_stock_list_v2(force_refresh=False)
    fc = FrozenCache()

    df_list = df_list.copy()
    df_list["Code"] = df_list["Code"].astype(str)
    mask = df_list["Code"].apply(lambda c: fc.has_all(c))
    rows = df_list[mask][["Code", "CompanyName", "MarketCode", "Sector33Name"]]

    tasks: list[tuple[str, str, str, str | None]] = []
    for row in rows.itertuples(index=False):
        code   = str(row.Code)
        name   = str(getattr(row, "CompanyName", "") or "")
        market = str(getattr(row, "MarketCode", "") or "")
        sector = str(getattr(row, "Sector33Name", "") or "") or None
        tasks.append((code, name, market, sector))
    return tasks

def lookup_company_name(session: requests.Session, code: str) -> str:
    fdm = FinancialDataManager(session)
    df_list = fdm.get_stock_list_v2(force_refresh=False)
    df_list = df_list.copy()
    df_list["Code"] = df_list["Code"].astype(str)
    hit = df_list[df_list["Code"] == str(code)]
    if not hit.empty:
        return str(hit.iloc[0].get("CompanyName") or "")
    return ""

# ------------------------------------------------------------
# ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ï¼ˆflatten / csv / mdï¼‰
# ------------------------------------------------------------
def _safe_bool(x) -> bool:
    return bool(x) if x is not None else False

def _extract_filters(d: dict) -> dict:
    f = d.get("filters") or {}
    return {
        "liquidity_ok": _safe_bool(f.get("liquidity_ok")),
        "market_cap_ok": _safe_bool(f.get("market_cap_ok")),
        "defensive_ps_ok": _safe_bool(f.get("defensive_ps_ok")),
        "per_satellite": _safe_bool(f.get("per_satellite")),
        "op_income_stable": _safe_bool(f.get("op_income_stable")),
        "op_income_reason": f.get("op_income_reason"),
        "base_ok": _safe_bool(f.get("base_ok")),
        "core_candidate": _safe_bool(f.get("core_candidate")),
        "satellite_candidate": _safe_bool(f.get("satellite_candidate")),
    }

def _flatten_result(d: dict) -> dict:
    pio = d.get("piotroski") or {}
    saf = d.get("safety") or {}
    spc = d.get("speculation") or {}
    safety_criteria = d.get("safety_criteria") or {}
    criteria = safety_criteria.get("criteria", {}) if isinstance(safety_criteria, dict) else {}
    flt = _extract_filters(d)

    return {
        "code": d.get("stock_code"),
        "name": d.get("company_name"),
        "sector": d.get("sector_name"),
        "price": d.get("current_price"),
        "ps": d.get("ps_ratio"),
        "peg": d.get("peg_ratio"),
        "per": d.get("per"),
        "rsi": d.get("rsi"),
        "adx": d.get("adx"),
        "below_ma200": d.get("below_ma200"),
        "piot": pio.get("score"),
        "piot_eval": pio.get("evaluation"),
        "safety": saf.get("total_score"),
        "safety_level": saf.get("safety_level"),
        "spec_score": spc.get("score"),
        "spec_level": spc.get("level"),
        "safety_criteria_score": safety_criteria.get("total_score") if isinstance(safety_criteria, dict) else None,
        "ps_under_1": criteria.get("ps_under_1", False),
        "cash_rich": criteria.get("cash_rich", False),
        "positive_ocf": criteria.get("positive_ocf", False),
        "equity_ratio_50plus": criteria.get("equity_ratio_50plus", False),
        "equity_ratio_70plus": criteria.get("equity_ratio_70plus", False),
        "growth_potential": criteria.get("growth_potential", False),
        "no_speculative_drop": criteria.get("no_speculative_drop", False),
        "equity_ratio": safety_criteria.get("equity_ratio") if isinstance(safety_criteria, dict) else None,
        "max_drawdown": safety_criteria.get("max_drawdown") if isinstance(safety_criteria, dict) else None,
        "sales_cagr": safety_criteria.get("sales_cagr") if isinstance(safety_criteria, dict) else None,
        "market_cap": d.get("market_cap"),
        "avg_volume_30d": d.get("avg_volume_30d"),
        **flt,
        "ok": d.get("success"),
        "error": d.get("error"),
    }

def _report_file_base(path: Path) -> str:
    """ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ­£è¦åŒ–ãƒ™ãƒ¼ã‚¹åã€‚top_recommended_core ã¨ top_recommended_* ã‚’åŒä¸€ã‚°ãƒ«ãƒ¼ãƒ—ã«"""
    stem = path.stem
    m = re.match(r"^(.+)_\d{8}_\d{6}$", stem)
    base = m.group(1) if m else stem
    if base.endswith("_core"):
        return base[:-5]
    return base

def _cleanup_old_files_by_ext(outdir: Path, ext: str) -> int:
    """æŒ‡å®šæ‹¡å¼µå­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¨®é¡ã”ã¨ã«æœ€æ–°1ä»¶ã ã‘æ®‹ã—ã€å¤ã„ã‚‚ã®ã‚’å‰Šé™¤ã€‚å‰Šé™¤ä»¶æ•°ã‚’è¿”ã™"""
    files = list(outdir.glob(f"*{ext}"))
    if not files:
        return 0
    by_base: Dict[str, list] = {}
    for f in files:
        base = _report_file_base(f)
        by_base.setdefault(base, []).append(f)
    deleted = 0
    for base, flist in by_base.items():
        if len(flist) <= 1:
            continue
        latest = max(flist, key=lambda p: p.stat().st_mtime)
        for f in flist:
            if f != latest:
                try:
                    f.unlink()
                    deleted += 1
                except OSError as e:
                    logger.warning("å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å¤±æ•—: %s (%s)", f, e)
    return deleted

def cleanup_old_report_files(outdir: Path) -> None:
    """å„ãƒ¬ãƒãƒ¼ãƒˆç¨®é¡ã”ã¨ã«æœ€æ–°1ä»¶ã ã‘æ®‹ã—ã€å¤ã„CSV/MD/JSONã‚’å‰Šé™¤ã—ã¦ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ç¯€ç´„ã™ã‚‹"""
    outdir = Path(outdir)
    if not outdir.exists():
        return
    total = 0
    total += _cleanup_old_files_by_ext(outdir, ".csv")
    total += _cleanup_old_files_by_ext(outdir, ".md")
    total += _cleanup_old_files_by_ext(outdir, ".json")
    if total > 0:
        print(f"ğŸ—‘ï¸ å¤ã„ãƒ¬ãƒãƒ¼ãƒˆ {total}ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ ({outdir})")

def write_candidate_sets(flat: pd.DataFrame, outdir: Path, timestamp: Optional[str] = None) -> list[Path]:
    outdir.mkdir(exist_ok=True, parents=True)
    ok = flat[flat["ok"] == True].copy()
    if ok.empty:
        return []

    core = ok[ok["core_candidate"] == True].copy()
    sat = ok[ok["satellite_candidate"] == True].copy()
    exc = ok[(ok["base_ok"] != True) | (ok["op_income_stable"] != True) | (ok["liquidity_ok"] != True) | (ok["market_cap_ok"] != True)].copy()

    # å›ºå®šåã§ä¸Šæ›¸ãï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„ï¼‰
    p_core = outdir / "core_candidates.csv"
    p_sat  = outdir / "satellite_candidates.csv"
    p_exc  = outdir / "excluded.csv"
    core.to_csv(p_core, index=False, encoding="utf-8-sig")
    sat.to_csv(p_sat, index=False, encoding="utf-8-sig")
    exc.to_csv(p_exc, index=False, encoding="utf-8-sig")

    summary = {
        "total_ok": int(len(ok)),
        "core": int(len(core)),
        "satellite": int(len(sat)),
        "excluded": int(len(exc)),
        "thresholds": {
            "MIN_AVG_VOLUME_30D": MIN_AVG_VOLUME_30D,
            "MIN_MARKET_CAP_JPY": MIN_MARKET_CAP_JPY,
            "MAX_PS_DEFENSIVE": MAX_PS_DEFENSIVE,
            "MAX_PER_CORE": MAX_PER_CORE,
            "OP_INCOME_YEARS": OP_INCOME_YEARS,
            "OP_INCOME_DROP_FLOOR": OP_INCOME_DROP_FLOOR,
            "EXCLUDE_OP_INCOME_DEFICIT": EXCLUDE_OP_INCOME_DEFICIT,
        }
    }
    p_sum = outdir / "filter_summary.json"
    p_sum.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
    return [p_core, p_sat, p_exc, p_sum]

def write_reports(flat: pd.DataFrame, outdir: Path, topn: int = 10, timestamp: Optional[str] = None) -> list[Path]:
    outdir.mkdir(exist_ok=True, parents=True)
    ok = flat[flat["ok"] == True].copy()
    if ok.empty:
        return []

    # å›ºå®šåã§ä¸Šæ›¸ãï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„ï¼‰
    for c in ["safety","piot","spec_score","per","peg","ps","rsi","adx","safety_criteria_score","market_cap","avg_volume_30d"]:
        if c in ok.columns:
            ok[c] = pd.to_numeric(ok[c], errors="coerce")

    core = ok[ok["core_candidate"] == True].copy()
    sat = ok[ok["satellite_candidate"] == True].copy()

    outs: list[Path] = []

    if not core.empty:
        p1 = outdir / "top_recommended_core.csv"
        core.sort_values(by=["safety","piot","spec_score"], ascending=[False,False,True]).head(topn).to_csv(p1, index=False, encoding="utf-8-sig")
        outs.append(p1)

        p2 = outdir / "top_safety_core.csv"
        core.sort_values(by=["safety","piot"], ascending=[False,False]).head(topn).to_csv(p2, index=False, encoding="utf-8-sig")
        outs.append(p2)

        p3 = outdir / "top_speculative_core.csv"
        core.sort_values(by=["spec_score"], ascending=False).head(topn).to_csv(p3, index=False, encoding="utf-8-sig")
        outs.append(p3)

        p4 = outdir / "top_piotroski_core.csv"
        core.sort_values(by=["piot","safety"], ascending=[False,False]).head(topn).to_csv(p4, index=False, encoding="utf-8-sig")
        outs.append(p4)

        if "safety_criteria_score" in core.columns:
            p5 = outdir / "top_safe_long_term_core.csv"
            core.sort_values(by=["safety_criteria_score"], ascending=False).head(topn).to_csv(p5, index=False, encoding="utf-8-sig")
            outs.append(p5)

    if not sat.empty:
        p6 = outdir / "top_recommended_satellite.csv"
        sat.sort_values(by=["safety","piot","spec_score"], ascending=[False,False,True]).head(topn).to_csv(p6, index=False, encoding="utf-8-sig")
        outs.append(p6)

    return outs

def write_markdown_report(flat: pd.DataFrame, outdir: Path, topn: int = 10, timestamp: Optional[str] = None) -> Optional[Path]:
    outdir.mkdir(exist_ok=True, parents=True)
    ok = flat[flat["ok"] == True].copy()
    core = ok[ok["core_candidate"] == True].copy()
    if core.empty:
        return None

    for c in ["safety","piot","spec_score","ps","per","market_cap","avg_volume_30d"]:
        if c in core.columns:
            core[c] = pd.to_numeric(core[c], errors="coerce")

    rec = core.sort_values(by=["safety","piot","spec_score"], ascending=[False,False,True]).head(topn)

    lines = ["# ãŠã™ã™ã‚ãƒˆãƒƒãƒ—ãƒ†ãƒ³ï¼ˆCoreå€™è£œï¼‰", ""]
    if timestamp:
        lines.append(f"**ç”Ÿæˆæ—¥æ™‚:** {timestamp.replace('_', ' ')}")
        lines.append("")
    lines.append(f"**ãƒ•ã‚£ãƒ«ã‚¿:** avg_volume_30d>={MIN_AVG_VOLUME_30D}, market_cap>={MIN_MARKET_CAP_JPY:,}JPY, PS<={MAX_PS_DEFENSIVE}, PER<={MAX_PER_CORE}, å–¶æ¥­åˆ©ç›Šå®‰å®š")
    lines.append("")
    for _, r in rec.iterrows():
        mc = r.get("market_cap")
        mc_str = f"{mc/1e9:.1f}B" if pd.notna(mc) else "N/A"
        vol = r.get("avg_volume_30d")
        vol_str = f"{int(vol):,}" if pd.notna(vol) else "N/A"
        lines.append(
            f"- **{r['code']} {r['name']}** | å®‰å…¨ {r['safety']} | Pio {r['piot']} | ä»•æ‰‹ {r['spec_score']} | "
            f"PER {r['per']} | PS {r['ps']} | æ™‚ä¾¡ç·é¡ {mc_str} | å‡ºæ¥é«˜(30d) {vol_str}"
        )

    p = outdir / f"report_core_top{topn}.md"
    p.write_text("\n".join(lines), encoding="utf-8")
    return p

# ==== æŠ•è³‡åŠ©è¨€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ====
def _grade_from_score(s: float) -> str:
    if s >= 85: return "A+"
    if s >= 75: return "A"
    if s >= 65: return "B+"
    if s >= 55: return "B"
    return "C"

def _val_score_from_ps_vs_sector(x: Optional[float]) -> float:
    if x is None or not np.isfinite(x): return 6.0
    if x <= 0.5: return 12.5
    if x <= 1.0: return 10.0
    if x <= 1.5: return 8.0
    if x <= 2.0: return 5.0
    if x <= 3.0: return 2.0
    return 0.0

def _val_score_from_peg(x: Optional[float]) -> float:
    """PEGãŒå–ã‚ŒãŸéŠ˜æŸ„ã®ã¿åŠ ç‚¹ã€‚0/æ¬ æã¯ä¸­ç«‹ï¼ˆèª¤åˆ¤å®šé˜²æ­¢ï¼‰"""
    if x is None or not np.isfinite(x): return 6.0
    if x <= 0: return 6.0
    if x <= 0.5: return 12.5
    if x <= 1.0: return 10.0
    if x <= 1.5: return 8.0
    if x <= 2.0: return 5.0
    if x <= 3.0: return 2.0
    return 0.0

def _tech_score(rsi: Optional[float], adx: Optional[float], below_ma200: Optional[bool]) -> float:
    r = 0.0
    if rsi is not None and np.isfinite(rsi):
        if 45 <= rsi <= 60: r += 15.0
        elif (40 <= rsi < 45) or (60 < rsi <= 70): r += 10.0
        elif (30 <= rsi < 40) or (70 < rsi <= 80): r += 5.0
        else: r += 0.0
    else:
        r += 7.5

    if adx is not None and np.isfinite(adx):
        if 20 <= adx <= 40: r += 10.0
        elif 15 <= adx < 20 or 40 < adx <= 50: r += 6.0
        elif 10 <= adx < 15 or 50 < adx <= 60: r += 3.0
        else: r += 0.0
    else:
        r += 5.0

    if below_ma200 is True:
        r = max(0.0, r - 3.0)
    return r

def _build_ranked(flat: pd.DataFrame) -> pd.DataFrame:
    df = flat.copy()
    for c in ["ps","peg","per","rsi","adx","piot","safety","spec_score","below_ma200"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    sec_med = df.groupby("sector")["ps"].median()

    def _ps_vs_sector(row):
        ps = row.get("ps")
        med = sec_med.get(row.get("sector"), np.nan)
        if pd.isna(ps) or pd.isna(med) or med <= 0:
            return np.nan
        return float(ps) / float(med)

    df["ps_vs_sector"] = df.apply(_ps_vs_sector, axis=1)

    df["valuation_ps"]  = df["ps_vs_sector"].apply(_val_score_from_ps_vs_sector)
    df["valuation_peg"] = df["peg"].apply(_val_score_from_peg)
    df["valuation_score"] = df["valuation_ps"] + df["valuation_peg"]                      # 0-25
    df["safety_score_scaled"] = df["safety"].fillna(12.0) * (20.0/25.0)                  # 0-20
    df["financial_score"] = df["piot"].fillna(4.5) * (22.5/9.0)                          # 0-22.5

    bm200 = df.get("below_ma200", pd.Series([np.nan]*len(df)))
    df["technical_score"] = [
        _tech_score(rsi, adx, (False if pd.isna(x) else bool(x)))
        for rsi, adx, x in zip(df["rsi"], df["adx"], bm200)
    ]  # 0-25

    df["spec_penalty"] = df["spec_score"].fillna(0.0).clip(lower=0, upper=100) * (10.0/100.0)  # 0-10
    df["per_penalty"] = np.where(df["per"].notna() & (df["per"] > MAX_PER_CORE), 3.0, 0.0)

    df["total_score"] = (df["valuation_score"] + df["safety_score_scaled"] +
                         df["financial_score"] + df["technical_score"] - df["spec_penalty"] - df["per_penalty"])
    df["total_score"] = df["total_score"].clip(lower=0, upper=100)
    df["grade"] = df["total_score"].apply(_grade_from_score)
    df["pio_disp"] = df["piot"].fillna(0).astype(int).astype(str) + "/9"
    return df

def write_investment_advice_report(flat: pd.DataFrame, outdir: Path,
                                   topn: int = 15, details_n: int = 30, timestamp: Optional[str] = None) -> list[Path]:
    outdir.mkdir(exist_ok=True, parents=True)

    ok = flat[flat["ok"] == True].copy()
    core = ok[ok["core_candidate"] == True].copy()
    sat = ok[ok["satellite_candidate"] == True].copy()
    if core.empty:
        return []

    ranked = _build_ranked(core)

    now = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    n = len(ranked)
    avg_score = ranked["total_score"].mean()
    grade_counts = ranked["grade"].value_counts().reindex(["A+","A","B+","B","C"]).fillna(0).astype(int)

    ps_avg = ranked["ps_vs_sector"].mean(skipna=True)
    ps_med = ranked["ps_vs_sector"].median(skipna=True)
    ps_min = ranked["ps_vs_sector"].min(skipna=True)
    ps_max = ranked["ps_vs_sector"].max(skipna=True)

    peg_avg = ranked["peg"].mean(skipna=True)
    peg_med = ranked["peg"].median(skipna=True)
    peg_min = ranked["peg"].min(skipna=True)
    peg_max = ranked["peg"].max(skipna=True)

    top = ranked.sort_values("total_score", ascending=False).head(topn)

    lines: list[str] = []
    lines.append("# ğŸ† PSãƒ»PEGãƒ¬ã‚·ã‚ªå¯¾å¿œ æŠ•è³‡éŠ˜æŸ„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ãƒ¬ãƒãƒ¼ãƒˆï¼ˆCoreå€™è£œï¼‰")
    lines.append("")
    lines.append(f"**ğŸ“… ç”Ÿæˆæ—¥æ™‚:** {now}")
    lines.append(f"**ğŸ“Š åˆ†æå¯¾è±¡(Core):** {n}éŠ˜æŸ„")
    lines.append("")
    lines.append("## âœ… å¿…é ˆãƒ•ã‚£ãƒ«ã‚¿ï¼ˆCoreå‰æï¼‰")
    lines.append("")
    lines.append(f"- avg_volume_30d >= {MIN_AVG_VOLUME_30D:,}")
    lines.append(f"- market_cap >= {MIN_MARKET_CAP_JPY:,} JPY")
    lines.append(f"- å–¶æ¥­åˆ©ç›Šå®‰å®šï¼ˆç›´è¿‘{OP_INCOME_YEARS}å¹´ãƒ»èµ¤å­—é™¤å¤–={EXCLUDE_OP_INCOME_DEFICIT}ãƒ»æ€¥è½floor={OP_INCOME_DROP_FLOOR}ï¼‰")
    lines.append(f"- PS <= {MAX_PS_DEFENSIVE}")
    lines.append(f"- PER <= {MAX_PER_CORE}ï¼ˆè¶…ã¯Satelliteæ‰±ã„ï¼‰")
    lines.append("")
    lines.append("## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼")
    lines.append("")
    lines.append(f"- **å¹³å‡æŠ•è³‡ã‚¹ã‚³ã‚¢:** {avg_score:.1f}ç‚¹")
    lines.append("- **ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ:**")
    lines.append(f"  - A+: {grade_counts['A+']}éŠ˜æŸ„")
    lines.append(f"  - A: {grade_counts['A']}éŠ˜æŸ„")
    lines.append(f"  - B+: {grade_counts['B+']}éŠ˜æŸ„")
    lines.append(f"  - B: {grade_counts['B']}éŠ˜æŸ„")
    lines.append(f"  - C: {grade_counts['C']}éŠ˜æŸ„")
    lines.append("")
    lines.append("## ğŸ’° ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ")
    lines.append("")
    lines.append("### PSãƒ¬ã‚·ã‚ªï¼ˆã‚»ã‚¯ã‚¿ãƒ¼æ¯”ï¼‰")
    lines.append(f"- å¹³å‡: {ps_avg:.2f}")
    lines.append(f"- ä¸­å¤®å€¤: {ps_med:.2f}")
    lines.append(f"- æœ€å°: {ps_min:.2f}")
    lines.append(f"- æœ€å¤§: {ps_max:.2f}")
    lines.append("")
    lines.append("### PEGãƒ¬ã‚·ã‚ª")
    lines.append(f"- å¹³å‡: {peg_avg:.2f}")
    lines.append(f"- ä¸­å¤®å€¤: {peg_med:.2f}")
    lines.append(f"- æœ€å°: {peg_min:.2f}")
    lines.append(f"- æœ€å¤§: {peg_max:.2f}")
    lines.append("")
    lines.append(f"## ğŸ† æŠ•è³‡æ¨å¥¨ Top{topn}éŠ˜æŸ„ï¼ˆCoreï¼‰")
    lines.append("")
    lines.append("| é †ä½ | éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ | éŠ˜æŸ„å | ã‚°ãƒ¬ãƒ¼ãƒ‰ | ã‚¹ã‚³ã‚¢ | ã‚»ã‚¯ã‚¿ãƒ¼ | PSæ¯” | PEG | ãƒ”ã‚ªãƒˆãƒ­ã‚¹ã‚­ãƒ¼ |")
    lines.append("|------|------------|--------|----------|--------|----------|------|-----|---------------|")
    for i, r in enumerate(top.itertuples(index=False), 1):
        ps_vs = 0 if pd.isna(r.ps_vs_sector) else r.ps_vs_sector
        peg = 0 if pd.isna(r.peg) else r.peg
        lines.append(f"| {i} | {r.code} | {r.name} | {r.grade} | {r.total_score:.1f} | {r.sector} | "
                     f"{ps_vs:.2f} | {peg:.2f} | {r.pio_disp} |")
    lines.append("")
    lines.append(f"## ğŸ“Š è©³ç´°åˆ†æï¼ˆä¸Šä½{details_n}éŠ˜æŸ„ï¼‰")
    lines.append("")
    detail_df = ranked.sort_values("total_score", ascending=False).head(details_n)
    for i, r in enumerate(detail_df.itertuples(index=False), 1):
        lines.append(f"### {i}. [{r.code}] {r.name} ({r.sector})")
        lines.append(f"**ç·åˆã‚¹ã‚³ã‚¢:** {r.total_score:.1f}ç‚¹ | **ã‚°ãƒ¬ãƒ¼ãƒ‰:** {r.grade}")
        lines.append("**ã‚¹ã‚³ã‚¢å†…è¨³:**")
        lines.append(f"- ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³: {r.valuation_score:.1f}ç‚¹")
        lines.append(f"- å®‰å…¨æ€§: {r.safety_score_scaled:.1f}ç‚¹")
        lines.append(f"- è²¡å‹™å¥å…¨æ€§: {r.financial_score:.1f}ç‚¹")
        lines.append(f"- ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«: {r.technical_score:.1f}ç‚¹")
        lines.append(f"- ä»•æ‰‹æ ªãƒšãƒŠãƒ«ãƒ†ã‚£: {r.spec_penalty:.1f}ç‚¹")
        lines.append(f"- PERãƒšãƒŠãƒ«ãƒ†ã‚£: {r.per_penalty:.1f}ç‚¹")
        lines.append("")

    if not sat.empty:
        lines.append("## ğŸ›° Satelliteå€™è£œï¼ˆå‚è€ƒï¼‰")
        lines.append("")
        lines.append(f"- ä»¶æ•°: {len(sat)}ï¼ˆbase_okã ãŒPS/PERæ¡ä»¶ã§coreå¤–ï¼‰")
        lines.append("")

    p_csv = outdir / "ranked_with_scores_core.csv"
    p_md  = outdir / "report_investment_advice_core.md"
    p_csv.write_text(ranked.to_csv(index=False, encoding="utf-8-sig"), encoding="utf-8")
    p_md.write_text("\n".join(lines), encoding="utf-8")
    return [p_csv, p_md]

# ------------------------------------------------------------
# â˜… è¿½åŠ : master CSVã‹ã‚‰ä¸€æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸è¦ï¼‰
# ------------------------------------------------------------
def _infer_timestamp_from_master_csv(path: Path) -> str:
    m = re.search(r"screening_offline_(\d{8}_\d{6})\.csv", path.name)
    if m:
        return m.group(1)
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

def generate_reports_from_master_csv(
    master_csv_path: str | Path,
    reports_dir: str | Path = REPORTS_DIR,
    topn: int = 30,
) -> list[Path]:
    master_csv_path = Path(master_csv_path)
    reports_dir = Path(reports_dir)
    reports_dir.mkdir(exist_ok=True, parents=True)

    if not master_csv_path.exists():
        raise FileNotFoundError(f"master_csv_path not found: {master_csv_path}")

    flat = pd.read_csv(master_csv_path, encoding="utf-8-sig")
    ts = _infer_timestamp_from_master_csv(master_csv_path)

    outputs: list[Path] = []
    outputs.append(master_csv_path)

    outputs += write_candidate_sets(flat, reports_dir, timestamp=ts)
    outputs += write_reports(flat, reports_dir, topn=topn, timestamp=ts)

    p_md = write_markdown_report(flat, reports_dir, topn=min(10, topn), timestamp=ts)
    if p_md:
        outputs.append(p_md)

    outputs += write_investment_advice_report(flat, reports_dir, topn=max(15, min(topn, 30)), details_n=max(30, topn), timestamp=ts)
    # å…¨ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å¾Œã«å¤ã„CSVã‚’å‰Šé™¤ï¼ˆoutput/ ã¨ output/reports/ ã®ä¸¡æ–¹ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç¯€ç´„ï¼‰
    cleanup_old_report_files(reports_dir)
    if reports_dir.parent != reports_dir:
        cleanup_old_report_files(reports_dir.parent)
    return outputs

# ------------------------------------------------------------
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–UI / CLI
# ------------------------------------------------------------
def run_interactive():
    session = get_authenticated_session_jquants()
    sector_avgs = DynamicSectorAverages(session).get_sector_averages()
    outdir = REPORTS_DIR
    outdir.mkdir(exist_ok=True, parents=True)

    while True:
        print("=== ãƒ¡ãƒ‹ãƒ¥ãƒ¼ ===")
        print("1) åé›†ï¼ˆä¾¡æ ¼+è²¡å‹™ã‚’å‡çµä¿å­˜ï¼‰")
        print("2) ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ä¸€æ‹¬åˆ†æï¼ˆcore/satellite/excluded å‡ºåŠ›ï¼‰")
        print("3) å˜éŠ˜æŸ„åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰")
        print("4) ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ã‚’æ›´æ–°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¨ˆç®—ï¼‰")
        print("5) å…¨éŠ˜æŸ„ã‚†ã£ãã‚Šåé›†ï¼ˆè‡ªå‹•å¾…æ©Ÿãƒ»å†é–‹å¯ï¼‰")
        print("6) é®®åº¦ã§å–ã‚Šç›´ã—åé›†ï¼ˆä¾‹: 7æ—¥ã‚ˆã‚Šå¤ã„ã‚‚ã®ã ã‘ï¼‰")
        print("7) å…¨éŠ˜æŸ„â€œå¼·åˆ¶â€å†åé›†ï¼ˆpendingåˆæœŸåŒ–ï¼‹å½“æ—¥å†å–å¾—ï¼‰")
        print("q) çµ‚äº†")
        choice = input("é¸æŠ: ").strip().lower()

        if choice == "1":
            budget = input("æœ¬æ—¥åé›†ã™ã‚‹éŠ˜æŸ„æ•°ï¼ˆæ¨å¥¨380ï¼‰: ").strip()
            budget = int(budget) if budget.isdigit() else 380
            s = collect_batch(session, budget)
            print(f"ğŸ“¦ åé›†: tried={s['tried']} ok={s['ok']} fail={s['fail']}")

        elif choice == "2":
            tasks = build_offline_analysis_tasks(session)
            if not tasks:
                print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸è¶³ã€‚å…ˆã«åé›†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                continue

            results = []
            max_workers = max(4, min(16, (os.cpu_count() or 4) * 2))
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futs = [ex.submit(
                    analyze_single_stock_complete_v3,
                    session, sector_avgs, code, name, market, sector,
                    offline=True
                ) for (code, name, market, sector) in tasks]
                for i, fut in enumerate(as_completed(futs), 1):
                    results.append(fut.result())
                    if i % 200 == 0 or i == len(futs):
                        ok_cnt = sum(1 for r in results if r.get("success"))
                        print(f"  â± {i}/{len(futs)} å®Œäº† (OK={ok_cnt})")

            flat = pd.DataFrame([_flatten_result(r) for r in results])
            master = outdir / "screening_offline.csv"
            flat.to_csv(master, index=False, encoding="utf-8-sig")

            outputs = generate_reports_from_master_csv(master, outdir, topn=30)
            print(f"âœ… å‡ºåŠ›: {master}")
            print(f"âœ… å‡ºåŠ›å…ˆ: {outdir}")
            print("=== ç”Ÿæˆç‰© ===")
            for p in outputs:
                print(f"  - {p}")

        elif choice == "3":
            code = input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰4æ¡: ").strip()
            name = lookup_company_name(session, code)
            res = analyze_single_stock_complete_v3(session, sector_avgs, code, name=name, offline=True)
            df = pd.DataFrame([_flatten_result(res)])
            fp = outdir / f"single_{code}.csv"
            df.to_csv(fp, index=False, encoding="utf-8-sig")
            cleanup_old_report_files(outdir)
            cleanup_old_report_files(outdir.parent)
            print(f"âœ… å‡ºåŠ›: {fp}")

        elif choice == "4":
            print("ğŸ“Š ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¨ˆç®—ä¸­...")
            sector_avgs_obj = DynamicSectorAverages(session)
            updated_avgs = sector_avgs_obj.calculate_sector_averages_from_cache()
            if updated_avgs:
                cache_file = CACHE_DIR / "sector_averages.json"
                sectors = ['è‡ªå‹•è»Š','åŠå°ä½“','é›»æ°—æ©Ÿå™¨','éŠ€è¡Œ','æƒ…å ±ãƒ»é€šä¿¡æ¥­','åŒ»è–¬å“','å•†ç¤¾','å°å£²','ã‚µãƒ¼ãƒ“ã‚¹','ã‚²ãƒ¼ãƒ ','åŒ–å­¦','ãã®ä»–']
                data = {}
                for sector in sectors:
                    if sector in updated_avgs:
                        data[sector] = updated_avgs[sector]
                    else:
                        data[sector] = sector_avgs_obj.get_default_sector_average(sector)
                cache_file.write_text(json.dumps({"timestamp": time.time(), "data": data}, ensure_ascii=False), encoding="utf-8")
                sector_avgs_obj.sector_cache = data
                sector_avgs_obj.cache_timestamp = time.time()
                sector_avgs = data
                print(f"âœ… ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆ{len(updated_avgs)}ã‚»ã‚¯ã‚¿ãƒ¼ï¼‰")
                for sector, stats in updated_avgs.items():
                    ps_val = stats.get('ps', None)
                    peg_val = stats.get('peg', None)
                    sample_count = stats.get('sample_count', 0)
                    ps_str = f"{ps_val:.2f}" if ps_val is not None else "N/A"
                    peg_str = f"{peg_val:.2f}" if peg_val is not None else "N/A"
                    print(f"  {sector}: PS={ps_str}, PEG={peg_str}, ã‚µãƒ³ãƒ—ãƒ«æ•°={sample_count}")
            else:
                print("âš ï¸ ã‚»ã‚¯ã‚¿ãƒ¼å¹³å‡ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        elif choice == "5":
            budget = input("1æ—¥ã‚ãŸã‚Šã®æœ€å¤§åé›†éŠ˜æŸ„æ•°ï¼ˆæ—¢å®š380ï¼‰: ").strip()
            budget = int(budget) if budget.isdigit() else 380
            collect_all_daemon(session, daily_budget=budget)

        elif choice == "6":
            days = input("ä½•æ—¥ã‚ˆã‚Šå¤ã‘ã‚Œã°å–ã‚Šç›´ã™ã‹ï¼ˆæ—¥æ•°ã€‚ä¾‹: 7ï¼‰: ").strip()
            days = int(days) if days.isdigit() else 7
            budget = input("1æ—¥ã‚ãŸã‚Šã®æœ€å¤§åé›†éŠ˜æŸ„æ•°ï¼ˆæ—¢å®š380ï¼‰: ").strip()
            budget = int(budget) if budget.isdigit() else 380
            collect_all_daemon(session, daily_budget=budget, refresh_days=days, reset_pending=True)

        elif choice == "7":
            budget = input("1æ—¥ã‚ãŸã‚Šã®æœ€å¤§åé›†éŠ˜æŸ„æ•°ï¼ˆæ—¢å®š380ï¼‰: ").strip()
            budget = int(budget) if budget.isdigit() else 380
            collect_all_daemon(session, daily_budget=budget, force_full=True, reset_pending=True)

        elif choice == "q":
            break

        else:
            print("ç„¡åŠ¹ãªé¸æŠ")

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--phase",
                        choices=["collect","collect_all","analyze","single","interactive"],
                        default="interactive")
    parser.add_argument("--code")
    parser.add_argument("--budget", type=int, default=380)
    parser.add_argument("--top", type=int, default=10)
    parser.add_argument("--reset-pending", action="store_true")
    parser.add_argument("--refresh-days", type=int)
    parser.add_argument("--force-full", action="store_true")
    args = parser.parse_args()

    if args.phase == "interactive":
        run_interactive()
        return

    session = get_authenticated_session_jquants()
    sector_avgs = DynamicSectorAverages(session).get_sector_averages()
    outdir = REPORTS_DIR
    outdir.mkdir(exist_ok=True, parents=True)

    if args.phase == "collect_all":
        collect_all_daemon(session,
                           daily_budget=args.budget,
                           refresh_days=args.refresh_days,
                           force_full=args.force_full,
                           reset_pending=args.reset_pending)
        return

    if args.phase == "collect":
        s = collect_batch(session, args.budget)
        print(f"ğŸ“¦ åé›†: tried={s['tried']} ok={s['ok']} fail={s['fail']}")
        return

    if args.phase == "single":
        if not args.code:
            raise SystemExit("--code å¿…é ˆ")
        name = lookup_company_name(session, args.code)
        res = analyze_single_stock_complete_v3(session, sector_avgs, args.code, name=name, offline=True)
        df = pd.DataFrame([_flatten_result(res)])
        fp = outdir / f"single_{args.code}.csv"
        df.to_csv(fp, index=False, encoding="utf-8-sig")
        cleanup_old_report_files(outdir)
        cleanup_old_report_files(outdir.parent)
        print(f"âœ… å˜éŠ˜æŸ„å‡ºåŠ›: {fp}")
        return

    if args.phase == "analyze":
        tasks = build_offline_analysis_tasks(session)
        if not tasks:
            print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸è¶³ã€‚å…ˆã« --phase collect ã‹ collect_all ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return

        results = []
        max_workers = max(4, min(16, (os.cpu_count() or 4) * 2))
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = [ex.submit(
                analyze_single_stock_complete_v3,
                session, sector_avgs, code, name, market, sector,
                offline=True
            ) for (code, name, market, sector) in tasks]
            for i, fut in enumerate(as_completed(futs), 1):
                results.append(fut.result())
                if i % 200 == 0 or i == len(futs):
                    ok_cnt = sum(1 for r in results if r.get("success"))
                    print(f"  â± {i}/{len(futs)} å®Œäº† (OK={ok_cnt})")

        flat = pd.DataFrame([_flatten_result(r) for r in results])
        master = outdir / "screening_offline.csv"
        flat.to_csv(master, index=False, encoding="utf-8-sig")

        outputs = generate_reports_from_master_csv(master, outdir, topn=max(10, args.top))
        print(f"âœ… ã‚ªãƒ•ãƒ©ã‚¤ãƒ³åˆ†æå‡ºåŠ›: {master}")
        print(f"âœ… å‡ºåŠ›å…ˆ: {outdir}")
        print("=== ç”Ÿæˆç‰© ===")
        for p in outputs:
            print(f"  - {p}")

if __name__ == "__main__":
    main()
